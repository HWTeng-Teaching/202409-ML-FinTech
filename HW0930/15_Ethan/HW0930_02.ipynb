{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1eLPOSr50F_xdZht8REnT8IT7S4eWwZSq","authorship_tag":"ABX9TyMxDfYrxVu7dw+bk9H2nfoR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#[PCA] 使用 'USArrests' 數據：問題解答與中英文解釋\n","#(a) 使用 PCA() 函數找出前四個主成分負荷向量 (Show the first to fourth principal component loadings vectors using PCA() function)\n","\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import statsmodels.api as sm\n","\n","# 從 statsmodels 中獲取 USArrests 數據\n","# Load the 'USArrests' dataset\n","df = sm.datasets.get_rdataset('USArrests', 'datasets').data\n","\n","# 標準化數據\n","# Standardize the data\n","scaler = StandardScaler()\n","df_scaled = scaler.fit_transform(df)\n","\n","# 進行 PCA 分析，取前四個主成分\n","# Perform PCA with the first four principal components\n","pca = PCA(n_components=4)\n","pca.fit(df_scaled)\n","\n","# 顯示前四個主成分的負荷向量\n","# Display the first four principal component loadings\n","loadings = pca.components_\n","print(loadings)\n","\n","\n","#使用 PCA() 函數來提取前四個主成分負荷向量，這些向量代表了數據集中每個變量對主成分的貢獻。\n","#used the PCA() function to extract the first four principal component loadings,\n","#which represent the contribution of each variable to the principal components.\n","\n","#(b) 使用 np.linalg.eig() 計算前四個主成分負荷向量 (Use np.linalg.eig() to find the first to #fourth principal component loadings vectors)\n","\n","import numpy as np\n","\n","# 計算協方差矩陣\n","# Compute the covariance matrix\n","cov_matrix = np.cov(df_scaled.T)\n","\n","# 使用 eig() 函數計算特徵值和特徵向量\n","# Use eig() function to compute eigenvalues and eigenvectors\n","eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n","\n","# 顯示特徵向量（主成分負荷向量）\n","# Display the eigenvectors (principal component loadings)\n","print(eig_vectors[:,:])\n","\n","#使用 np.linalg.eig() 函數從協方差矩陣中提取特徵值和特徵向量，這些特徵向量也就是主成分負荷向量。\n","#used the np.linalg.eig() function to extract the eigenvalues and eigenvectors from the covariance matrix.\n","#The eigenvectors are the principal component loadings.\n","\n","#(c) 使用 np.linalg.svd() 計算前四個主成分負荷向量\n","#(Use np.linalg.svd() to find the first to fourth principal component loadings vectors)\n","# 使用 SVD 分解數據矩陣\n","# Perform Singular Value Decomposition (SVD)\n","U, S, Vt = np.linalg.svd(df_scaled)\n","\n","# 顯示 V 的轉置（即主成分負荷向量）\n","# Display V transpose (principal component loadings)\n","print(Vt[:])\n","\n","#使用 SVD 方法分解標準化後的數據矩陣，我們可以從 V 的轉置中獲得主成分負荷向量。\n","\n","#使用 SVD 方法分解標準化後的數據矩陣，我們可以從 V 的轉置中獲得主成分負荷向量。\n","#Using Singular Value Decomposition (SVD),\n","#we decompose the standardized data matrix and extract the principal component loadings from the transpose of 𝑉.\n","\n","#(d) 結果分析 (Analysis of Results)\n","#問題：這些結果 (a), (b), 和 (c) 是否完全相同？為什麼？\n","#(Are the results from (a), (b), and (c) exactly the same? Why or why not?)\n","\n","#是的，儘管負荷向量的符號和排列可能不同，但它們在數學上是等價的。這是因為 PCA 中的主成分負荷向量可以翻轉其符號，這不會改變其解釋。\n","#Yes, although the signs and arrangement of the loadings might differ, they are mathematically equivalent.\n","#This is because in PCA, principal component loadings can have their signs flipped without affecting the interpretation."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pbtQN2D1Tfz","executionInfo":{"status":"ok","timestamp":1728218766625,"user_tz":-480,"elapsed":361,"user":{"displayName":"ES","userId":"04782574932703245932"}},"outputId":"1eb31d07-d49f-4c6a-ff6a-1b2779e8ce9b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.53589947  0.58318363  0.27819087  0.54343209]\n"," [-0.41818087 -0.1879856   0.87280619  0.16731864]\n"," [-0.34123273 -0.26814843 -0.37801579  0.81777791]\n"," [-0.6492278   0.74340748 -0.13387773 -0.08902432]]\n","[[ 0.53589947  0.41818087  0.6492278  -0.34123273]\n"," [ 0.58318363  0.1879856  -0.74340748 -0.26814843]\n"," [ 0.27819087 -0.87280619  0.13387773 -0.37801579]\n"," [ 0.54343209 -0.16731864  0.08902432  0.81777791]]\n","[[-0.53589947 -0.58318363 -0.27819087 -0.54343209]\n"," [-0.41818087 -0.1879856   0.87280619  0.16731864]\n"," [ 0.34123273  0.26814843  0.37801579 -0.81777791]\n"," [ 0.6492278  -0.74340748  0.13387773  0.08902432]]\n"]}]}]}