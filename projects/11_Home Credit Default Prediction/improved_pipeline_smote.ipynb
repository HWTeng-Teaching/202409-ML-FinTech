{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee21a46-4fde-420e-9cfa-7995cf83444a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /opt/conda/lib/python3.10/site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (24.12.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.25.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/conda/lib/python3.10/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.11.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (1.4.2)\n",
      "Methods available in SMOTE object:\n",
      "['__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__sklearn_tags__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_build_request_for_signature', '_check_X_y', '_check_feature_names', '_check_n_features', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_estimator_type', '_fit_resample', '_generate_samples', '_get_default_requests', '_get_doc_link', '_get_metadata_request', '_get_param_names', '_get_tags', '_in_danger_noise', '_make_samples', '_more_tags', '_parameter_constraints', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_sampling_strategy_docstring', '_sampling_type', '_validate_data', '_validate_estimator', '_validate_params', 'fit', 'fit_resample', 'get_feature_names_out', 'get_metadata_routing', 'get_params', 'k_neighbors', 'random_state', 'sampling_strategy', 'set_params']\n",
      "Does 'fit' method exist? True\n",
      "Does 'transform' method exist? False\n",
      "Name: imbalanced-learn\n",
      "Version: 0.13.0\n",
      "Summary: Toolbox for imbalanced dataset in machine learning\n",
      "Home-page: https://imbalanced-learn.org/\n",
      "Author: \n",
      "Author-email: \"G. Lemaitre\" <g.lemaitre58@gmail.com>, \"C. Aridas\" <ichkoar@gmail.com>\n",
      "License: \n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: joblib, numpy, scikit-learn, scipy, sklearn-compat, threadpoolctl\n",
      "Required-by: imblearn\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages using pip\n",
    "!pip install scikit-optimize\n",
    "!pip install imbalanced-learn\n",
    "!pip install imblearn\n",
    "!pip install seaborn\n",
    "!pip install xgboost\n",
    "!pip install joblib\n",
    "\n",
    "from os import pipe\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from skopt import BayesSearchCV  # Import BayesianSearchCV\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import Memory\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Check if 'fit' and 'transform' methods exist\n",
    "print(\"Methods available in SMOTE object:\")\n",
    "print(dir(smote))\n",
    "\n",
    "# Check if fit and transform exist in the SMOTE object\n",
    "print(\"Does 'fit' method exist?\", hasattr(smote, 'fit'))\n",
    "print(\"Does 'transform' method exist?\", hasattr(smote, 'transform'))\n",
    "!pip show imbalanced-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e6ab0-8104-46e1-b201-73e38ca7c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       275\n",
      "           1       0.53      0.76      0.62        25\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.75      0.85      0.79       300\n",
      "weighted avg       0.94      0.92      0.93       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(enable_metadata_routing=False)\n",
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, \n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline with SMOTE and a RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26573d9d-5a9a-4193-8e87-057628a0833b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (307511, 122)\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
      "dtypes: float64(65), int64(41), object(16)\n",
      "memory usage: 286.2+ MB\n",
      "None\n",
      "[PCA - Logistic Regression] Training started...\n",
      "[PCA - Logistic Regression] Performing Bayesian search...\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define cache directory\n",
    "cache_dir = './pipeline_cache'\n",
    "memory = Memory(location=cache_dir, verbose=0)\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(enable_metadata_routing=False)\n",
    "# prompt: make another where target variable TARGET is excluded\n",
    "\n",
    "df = pd.read_csv('application_train.csv')\n",
    "print(\"Data Shape:\", df.shape)\n",
    "print(\"\\nData Info:\")\n",
    "print(df.info())\n",
    "\n",
    "X = df.drop('TARGET', axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "param_distributions = {\n",
    "    'Logistic Regression': {\n",
    "        'C': Real(1e-5, 100, prior='log-uniform'),\n",
    "        'max_iter': Categorical([300]),\n",
    "       # 'n_jobs': Categorical([-1]),\n",
    "        'solver': Categorical(['lbfgs', 'saga', 'liblinear', 'newton-cg']),\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': Integer(5, 150),\n",
    "        'max_depth': Integer(5, 100),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "        'bootstrap': Categorical([True, False]),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        'class_weight': Categorical([None, 'balanced']),\n",
    "        'n_jobs': Categorical([-1]),\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': Integer(1, 200),\n",
    "        'weights': Categorical(['uniform', 'distance']),\n",
    "        'leaf_size': Integer(10, 100),\n",
    "        'p': Integer(1, 5),\n",
    "        'n_jobs': Categorical([-1]),\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'subsample': Real(0.3, 1.0),\n",
    "        'n_estimators': Integer(5, 150),\n",
    "        'max_depth': Integer(3, 15),\n",
    "        'learning_rate': Real(0.001, 0.5, prior='log-uniform'),\n",
    "        'gamma': Real(0, 1.0),\n",
    "        'colsample_bytree': Real(0.3, 1.0),\n",
    "        'reg_alpha': Real(0, 10),\n",
    "        'reg_lambda': Real(0, 10),\n",
    "        'scale_pos_weight': Real(1, 10),\n",
    "        'n_jobs': Categorical([-1]),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "# Preprocessing and transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "]\n",
    "                                 #, memory=memory\n",
    "                                )  # Enable caching\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "]\n",
    "                                  # , memory=memory\n",
    "                                  )  # Enable caching\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create pipelines with caching enabled\n",
    "def create_pipelines():\n",
    "    pre_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "    ])\n",
    "\n",
    "    smote_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "    ])\n",
    "\n",
    "    rfe_pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', RFE(estimator=LogisticRegression(max_iter=150, random_state=42, n_jobs=-1), n_features_to_select=10)),\n",
    "    ], memory=memory)\n",
    "    \n",
    "    pca_pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('dim_reduction', PCA(n_components=71, random_state=42)),\n",
    "    ])\n",
    "\n",
    "    rfe_smote_pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('feature_selection', RFE(estimator=LogisticRegression(max_iter=150, random_state=42, n_jobs=-1), n_features_to_select=10)),\n",
    "    ], memory=memory)\n",
    "\n",
    "    pca_smote_pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('dim_reduction', PCA(n_components=71, random_state=42)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"PCA\": pca_pipeline,\n",
    "        \"PCA_SMOTE\": pca_smote_pipeline,\n",
    "        \"RFE\": rfe_pipeline, \n",
    "        \"RFE_SMOTE\": rfe_smote_pipeline,\n",
    "        \"Preprocessing\": pre_pipeline, \n",
    "        \"SMOTE\": smote_pipeline\n",
    "        }\n",
    "\n",
    "import concurrent.futures\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.pipeline import Pipeline as ImbPipeline\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "# Model evaluation function without parallelization\n",
    "def evaluate_pipelines(models, pipelines, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    def evaluate_pipeline_model(pipeline_name, pipeline, model_name, model):\n",
    "        print(f\"[{pipeline_name} - {model_name}] Training started...\")\n",
    "\n",
    "        best_model = None\n",
    "        pipeline_with_model = None\n",
    "        if param_distributions and model_name in param_distributions:\n",
    "            print(f\"[{pipeline_name} - {model_name}] Performing Bayesian search...\")\n",
    "            bayes_search = BayesSearchCV(\n",
    "                model,\n",
    "                param_distributions[model_name],\n",
    "                n_iter=60,\n",
    "                n_jobs=20,\n",
    "                n_points=12,\n",
    "                cv=2,\n",
    "                random_state=42,\n",
    "                scoring=make_scorer(f1_score, pos_label=1),  # F1 score for the minority class (class 1)\n",
    "                verbose=5\n",
    "            )\n",
    "            pipeline_with_model = Pipeline(steps=pipeline.steps + [('classifier', bayes_search)])\n",
    "            pipeline_with_model.fit(X_train, y_train)\n",
    "            best_model = bayes_search.best_estimator_\n",
    "            print(f\"[{pipeline_name} - {model_name}] Best hyperparameters: {bayes_search.best_params_}\")\n",
    "        else:\n",
    "            print(f\"[{pipeline_name} - {model_name}] No hyperparameter tuning.\")\n",
    "            pipeline_with_model = Pipeline(steps=pipeline.steps + [('classifier', model)])\n",
    "            pipeline_with_model.fit(X_train, y_train)\n",
    "            best_model = model\n",
    "        \n",
    "\n",
    "        #print(f\"[{pipeline_name} - {model_name}] Using default parameters...\")\n",
    "        #pipeline_with_model = Pipeline(steps=pipeline.steps[:-1] + [('classifier', model)])\n",
    "        #pipeline_with_model.fit(X_train, y_train)\n",
    "        #best_model = model\n",
    "\n",
    "\n",
    "        print(f\"[{pipeline_name} - {model_name}] Evaluating model...\")\n",
    "        pipeline = pipeline_with_model\n",
    "        # Predictions and metrics\n",
    "        y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "        print(f\"[{pipeline_name} - {model_name}] Accuracy: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"[{pipeline_name} - {model_name}] Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "        print(f\"[{pipeline_name} - {model_name}] Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "        # Store results\n",
    "        key = f\"{model_name}_{pipeline_name}\"\n",
    "        results[key] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"Classification Report\": classification_report(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{key} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Sequential execution of each pipeline-model combination\n",
    "    for pipeline_name, pipeline in pipelines.items():\n",
    "        for model_name, model in models.items():\n",
    "            evaluate_pipeline_model(pipeline_name, pipeline, model_name, model)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Models and Pipelines')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Create pipelines\n",
    "pipelines = create_pipelines()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Evaluate models with pipelines\n",
    "results = evaluate_pipelines(models, pipelines, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "for key, metrics in results.items():\n",
    "    print(f\"\\n{key} Performance:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
    "#    print(f\"CV Mean ROC AUC: {metrics['CV Mean ROC AUC']:.4f} (+/- {metrics['CV Std ROC AUC']:.4f})\")\n",
    "    print(f\"Classification Report:\\n{metrics['Classification Report']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efe6f50-1f70-49fd-abcc-134af1428e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       275\n",
      "           1       0.53      0.76      0.62        25\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.75      0.85      0.79       300\n",
      "weighted avg       0.94      0.92      0.93       300\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1dff39-bb12-4496-bf81-697df3d358ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a110a-54ed-4d3f-860d-7e6df2b63ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d45bf-5659-44ff-9bac-4b5a25b5e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5f94c-9819-4137-9981-648097aa76d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd495d-7cc9-49ec-9c1b-cbec964b8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Shape: (307511, 122)\n",
    "\n",
    "Data Info:\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 307511 entries, 0 to 307510\n",
    "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
    "dtypes: float64(65), int64(41), object(16)\n",
    "memory usage: 286.2+ MB\n",
    "None\n",
    "[PCA - Logistic Regression] Training started...\n",
    "[PCA - Logistic Regression] Performing Bayesian search...\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "[PCA - Logistic Regression] Best hyperparameters: OrderedDict([('C', 99.52032142391181), ('max_iter', 300), ('solver', 'lbfgs')])\n",
    "[PCA - Logistic Regression] Evaluating model...\n",
    "[PCA - Logistic Regression] Accuracy: 0.9193, ROC AUC: 0.7419\n",
    "[PCA - Logistic Regression] Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     56538\n",
    "           1       0.51      0.01      0.02      4965\n",
    "\n",
    "    accuracy                           0.92     61503\n",
    "   macro avg       0.72      0.50      0.49     61503\n",
    "weighted avg       0.89      0.92      0.88     61503\n",
    "\n",
    "[PCA - Logistic Regression] Confusion Matrix:\n",
    "[[56501    37]\n",
    " [ 4926    39]]\n",
    "[PCA - Random Forest] Training started...\n",
    "[PCA - Random Forest] Performing Bayesian search...\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
    "[PCA - Random Forest] Best hyperparameters: OrderedDict([('bootstrap', True), ('class_weight', 'balanced'), ('max_depth', 7), ('max_features', 'log2'), ('min_samples_leaf', 6), ('min_samples_split', 20), ('n_estimators', 83), ('n_jobs', -1)])\n",
    "[PCA - Random Forest] Evaluating model...\n",
    "[PCA - Random Forest] Accuracy: 0.7004, ROC AUC: 0.7130\n",
    "[PCA - Random Forest] Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.71      0.81     56538\n",
    "           1       0.15      0.61      0.25      4965\n",
    "\n",
    "    accuracy                           0.70     61503\n",
    "   macro avg       0.55      0.66      0.53     61503\n",
    "weighted avg       0.89      0.70      0.77     61503\n",
    "\n",
    "[PCA - Random Forest] Confusion Matrix:\n",
    "[[40074 16464]\n",
    " [ 1960  3005]]\n",
    "[PCA - K-Nearest Neighbors] Training started...\n",
    "[PCA - K-Nearest Neighbors] Performing Bayesian search...\n",
    "Fitting 2 folds for each of 12 candidates, totalling 24 fits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
