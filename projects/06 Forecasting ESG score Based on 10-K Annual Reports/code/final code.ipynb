{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Step 1: Perform PCA for dimensionality reduction\n",
    "def perform_pca(embeddings, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"PCA explained variance: {explained_variance:.2%}\")\n",
    "    return reduced_embeddings\n",
    "\n",
    "# Step 2: Perform t-SNE for dimensionality reduction\n",
    "def perform_tsne(embeddings, n_components=2, perplexity=30, random_state=42):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=random_state)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    return reduced_embeddings\n",
    "\n",
    "# Step 3: Visualize embeddings in 2D\n",
    "def plot_embeddings_2d(embeddings_2d, labels=None, title=\"2D Embedding Visualization\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    if labels is not None:\n",
    "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "        plt.colorbar()\n",
    "    else:\n",
    "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 4: Perform K-Means Clustering\n",
    "def perform_kmeans(embeddings, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return labels, kmeans\n",
    "\n",
    "# Step 5: Compute Cosine Similarity Matrix\n",
    "def compute_cosine_similarity(embeddings):\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Step 6: Compute Wordcloud\n",
    "def generate_wordcloud(tfidf_matrix, vectorizer, max_words=100, title=\"Word Cloud\"):\n",
    "    \"\"\"\n",
    "    Generates a word cloud based on TF-IDF scores.\n",
    "    \"\"\"\n",
    "    # Compute the average TF-IDF score for each term\n",
    "    tfidf_scores = np.mean(tfidf_matrix, axis=0)\n",
    "    tfidf_scores_dict = {term: tfidf_scores[idx] for term, idx in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\", max_words=max_words)\n",
    "    wordcloud.generate_from_frequencies(tfidf_scores_dict)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Squared Errors for each fold: [312.01108056 166.4707854  436.07177085 245.28907707 255.92014289]\n",
      "Average Cross-Validation MSE: 283.1525713539246\n",
      "Mean Squared Error: 78.75377610617883\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths\n",
    "x_folder_path = r\"C:\\Users\\sabri\\OneDrive\\桌面\\論文\\論文code\\Air transportation SIC 4512\\ML dataset\\2013-2023_all (X)\"\n",
    "y_file_path = r\"C:\\Users\\sabri\\OneDrive\\桌面\\論文\\論文code\\Air transportation SIC 4512\\ML dataset\\Annual ESG score (Y)\\Combined_ESG_Scores.csv\"\n",
    "test_file_path = r\"C:\\Users\\sabri\\OneDrive\\桌面\\論文\\論文code\\Air transportation SIC 4512\\ML dataset\\Test data\"\n",
    "esg_file_path = r\"C:\\Users\\sabri\\OneDrive\\桌面\\論文\\論文code\\Air transportation SIC 4512\\ESG wordlist.csv\"  # Path to the ESG wordlist\n",
    "\n",
    "\n",
    "# Load ESG dictionary\n",
    "esg_dict = pd.read_csv(esg_file_path)\n",
    "esg_terms = set(esg_dict['terms'].str.lower())  # Convert to lowercase for case-insensitive matching\n",
    "\n",
    "# Load Y data (Annual ESG scores)\n",
    "y_data = pd.read_csv(y_file_path)\n",
    "esg_scores = y_data['ESG'].tolist()  # Assuming the column name is 'ESG'\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.encode().decode(\"unicode_escape\")  # Decode any Unicode escape sequences\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word in esg_terms]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "    \n",
    "    \n",
    "# Step 1: Extract, preprocess, and store content from each file in the X folder\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(x_folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(x_folder_path, filename), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Extract content (assuming the entire content is relevant)\n",
    "            content = json.dumps(data)  # Convert JSON to string\n",
    "            # Preprocess the content\n",
    "            cleaned_text = preprocess_text(content)\n",
    "            documents.append(cleaned_text)\n",
    "\n",
    "# documents\n",
    "\n",
    "# Step 2: Generate TF-IDF vectors for the documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Step 3: Train a model to predict scores based on TF-IDF vectors\n",
    "X = tfidf_matrix\n",
    "y = esg_scores\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use Linear Regression for prediction\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # 5-fold cross-validation\n",
    "cv_scores = -cv_scores  # Convert to positive MSE for interpretation\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Cross-Validation Mean Squared Errors for each fold:\", cv_scores)\n",
    "print(\"Average Cross-Validation MSE:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Step 4: Define a function for predicting scores on new text input\n",
    "def predict_score(new_text):\n",
    "    new_text = preprocess_text(new_text)\n",
    "    new_tfidf = vectorizer.transform(new_text).toarray()\n",
    "    predicted_score = model.predict(new_tfidf)\n",
    "    return predicted_score[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1: Predicted ESG Score: 93.31725305558422\n",
      "File 2: Predicted ESG Score: 70.30944193096272\n",
      "File 3: Predicted ESG Score: 70.5790152910135\n",
      "File 4: Predicted ESG Score: 55.260703835662866\n",
      "File 5: Predicted ESG Score: 58.160265538596036\n",
      "File 6: Predicted ESG Score: 40.775688433261735\n",
      "File 7: Predicted ESG Score: 55.64681443679913\n",
      "File 8: Predicted ESG Score: 86.63824984621135\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Process and predict ESG scores for test data\n",
    "new_documents = []\n",
    "\n",
    "for filename in os.listdir(test_file_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(test_file_path, filename), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Extract content (assuming the entire content is relevant)\n",
    "            content = json.dumps(data)  # Convert JSON to string\n",
    "            # Preprocess the content\n",
    "            new_cleaned_text = preprocess_text(content)\n",
    "            new_documents.append(new_cleaned_text)\n",
    "\n",
    "# Transform the new documents using the fitted vectorizer\n",
    "new_tfidf = vectorizer.transform(new_documents).toarray()\n",
    "\n",
    "# Predict ESG scores for the new files\n",
    "predicted_scores = model.predict(new_tfidf)\n",
    "\n",
    "# Print the predicted ESG scores\n",
    "for idx, score in enumerate(predicted_scores, start=1):\n",
    "    print(f\"File {idx}: Predicted ESG Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection with forward stepwise method (include cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with Forward Stepwise Test MSE: 317.67948526010025\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "# Forward stepwise feature selection\n",
    "sfs_lr = SequentialFeatureSelector(model_lr, n_features_to_select='auto', direction='forward', cv=5)\n",
    "sfs_lr.fit(X, y)\n",
    "\n",
    "# Transform the dataset to selected features\n",
    "X_selected_lr = sfs_lr.transform(X)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_selected_lr, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model_lr.fit(X_train_lr, y_train_lr)\n",
    "y_pred_lr = model_lr.predict(X_test_lr)\n",
    "mse_lr = mean_squared_error(y_test_lr, y_pred_lr)\n",
    "print(\"Linear Regression with Forward Stepwise Test MSE:\", mse_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression with Forward Stepwise Test MSE: 39.79698498222867\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "model_lasso.fit(X, y)\n",
    "\n",
    "# Transform the dataset to selected features\n",
    "X_selected_lasso = model_lasso.transform(X)\n",
    "X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(X_selected_lasso, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model_lasso.fit(X_train_lasso, y_train_lasso)\n",
    "y_pred_lasso = model_lasso.predict(X_test_lasso)\n",
    "mse_lasso = mean_squared_error(y_test_lasso, y_pred_lasso)\n",
    "print(\"Lasso Regression with Forward Stepwise Test MSE:\", mse_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with Forward Stepwise Test MSE: 101.61288628555778\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "model_tree = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "# Forward stepwise feature selection\n",
    "sfs_tree = SequentialFeatureSelector(model_tree, n_features_to_select='auto', direction='forward', cv=5)\n",
    "sfs_tree.fit(X, y)\n",
    "\n",
    "# Transform the dataset to selected features\n",
    "X_selected_tree = sfs_tree.transform(X)\n",
    "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_selected_tree, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model_tree.fit(X_train_tree, y_train_tree)\n",
    "y_pred_tree = model_tree.predict(X_test_tree)\n",
    "mse_tree = mean_squared_error(y_test_tree, y_pred_tree)\n",
    "print(\"Decision Tree with Forward Stepwise Test MSE:\", mse_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection with decision tree and fit it in XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [  0   2   3   7   8  11  16  23  26  28  29  33  34  36  37  40  41  43\n",
      "  45  48  50  52  53  61  62  65  68  69  71  72  73  78  80  85  88  90\n",
      "  92  93  94  96  97  98  99 102 105 107 109 113 115 118 119 120 121 123\n",
      " 124 126 129 131 133 134 135 136 137 138 142 143 145 146 148 151 152 153\n",
      " 155 157 159 161 163 168 169 170 175 176 177 178 180 181 184 185 186 188\n",
      " 189 190 191 192 194 195 196 197 198 200 201 202 203 204 207 208 209 211\n",
      " 212 213 214 216 219 220 222 224 225 227 234 235 236 238 239 240 243 244\n",
      " 245 246 252 257 258 265 267 270 272 273 275 277 278 279 281 284 285 286\n",
      " 287 292 294 295 297 300 303 304 307 311 312 314 316 317 318 319]\n",
      "XGBoost Test MSE with Selected Features: 46.0106124622111\n"
     ]
    }
   ],
   "source": [
    "# Get indices of selected features\n",
    "selected_feature_indices = sfs_tree.get_support(indices=True)\n",
    "print(f\"Selected Feature Indices: {selected_feature_indices}\")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Train XGBoost on the selected features\n",
    "model_xgb = XGBRegressor(n_estimators=50, max_depth=5, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model_xgb.fit(X_train_tree, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = model_xgb.predict(X_test_tree)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Test MSE with Selected Features: {mse_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full XGboost Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Forward stepwise feature selection\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sfs_xgb \u001b[38;5;241m=\u001b[39m SequentialFeatureSelector(model_xgb, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m sfs_xgb\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Transform the dataset to selected features\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_selected_xgb \u001b[38;5;241m=\u001b[39m sfs_xgb\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:255\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    253\u001b[0m is_auto_select \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_to_select \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 255\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_best_new_feature_score(\n\u001b[0;32m    256\u001b[0m         cloned_estimator, X, y, cv, current_mask\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:286\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, cv, current_mask)\u001b[0m\n\u001b[0;32m    284\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[0;32m    285\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 286\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m    287\u001b[0m         estimator,\n\u001b[0;32m    288\u001b[0m         X_new,\n\u001b[0;32m    289\u001b[0m         y,\n\u001b[0;32m    290\u001b[0m         cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    291\u001b[0m         scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring,\n\u001b[0;32m    292\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    293\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    294\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    713\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    714\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    715\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    716\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    717\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    718\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    719\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    720\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    721\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    722\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    723\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    724\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[0;32m    426\u001b[0m         X,\n\u001b[0;32m    427\u001b[0m         y,\n\u001b[0;32m    428\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    429\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    430\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    431\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    432\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    433\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    434\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    435\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    436\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    438\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1109\u001b[0m     params,\n\u001b[0;32m   1110\u001b[0m     train_dmatrix,\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1112\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1113\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1114\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1115\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1116\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1117\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1118\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:159\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    156\u001b[0m metric_fn \u001b[38;5;241m=\u001b[39m _configure_custom_metric(feval, custom_metric)\n\u001b[0;32m    157\u001b[0m evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(evals) \u001b[38;5;28;01mif\u001b[39;00m evals \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m--> 159\u001b[0m bst \u001b[38;5;241m=\u001b[39m Booster(params, [dtrain] \u001b[38;5;241m+\u001b[39m [d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals], model_file\u001b[38;5;241m=\u001b[39mxgb_model)\n\u001b[0;32m    160\u001b[0m start_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose_eval:\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1710\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, cache, model_file)\u001b[0m\n\u001b[0;32m   1703\u001b[0m _check_call(\n\u001b[0;32m   1704\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mXGBoosterCreate(\n\u001b[0;32m   1705\u001b[0m         dmats, c_bst_ulong(\u001b[38;5;28mlen\u001b[39m(cache)), ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m )\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m   1709\u001b[0m     \u001b[38;5;66;03m# Validate feature only after the feature names are saved into booster.\u001b[39;00m\n\u001b[1;32m-> 1710\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(d)\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_file, Booster):\n\u001b[0;32m   1713\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:3029\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assign_dmatrix_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: DMatrix) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mnum_row() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3030\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   3032\u001b[0m     fn \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_names\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1239\u001b[0m, in \u001b[0;36mDMatrix.num_row\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of rows in the DMatrix.\"\"\"\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m ret \u001b[38;5;241m=\u001b[39m c_bst_ulong()\n\u001b[1;32m-> 1239\u001b[0m _check_call(_LIB\u001b[38;5;241m.\u001b[39mXGDMatrixNumRow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mbyref(ret)))\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "model_xgb = XGBRegressor(n_estimators=10, max_depth=5, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Forward stepwise feature selection\n",
    "sfs_xgb = SequentialFeatureSelector(model_xgb, n_features_to_select='auto', direction='forward', cv=3)\n",
    "sfs_xgb.fit(X, y)\n",
    "\n",
    "# Transform the dataset to selected features\n",
    "X_selected_xgb = sfs_xgb.transform(X)\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_selected_xgb, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "y_pred_xgb = model_xgb.predict(X_test_xgb)\n",
    "mse_xgb = mean_squared_error(y_test_xgb, y_pred_xgb)\n",
    "print(\"XGBoost with Forward Stepwise Test MSE:\", mse_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine version of 2024 ESG score prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 320 features, but Lasso is expecting 160 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m new_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(new_documents)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Predict ESG scores using all models\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m predicted_scores_lasso \u001b[38;5;241m=\u001b[39m model_lasso\u001b[38;5;241m.\u001b[39mpredict(new_tfidf)\n\u001b[0;32m     16\u001b[0m predicted_scores_tree \u001b[38;5;241m=\u001b[39m model_tree\u001b[38;5;241m.\u001b[39mpredict(new_tfidf)\n\u001b[0;32m     17\u001b[0m predicted_scores_xgb \u001b[38;5;241m=\u001b[39m model_xgb\u001b[38;5;241m.\u001b[39mpredict(new_tfidf)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:306\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1147\u001b[0m, in \u001b[0;36mElasticNet._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:285\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    283\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    286\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coef_\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\sabri\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 320 features, but Lasso is expecting 160 features as input."
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "new_documents = []\n",
    "for filename in os.listdir(test_file_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(test_file_path, filename), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            content = json.dumps(data)  # Convert JSON to string\n",
    "            new_cleaned_text = preprocess_text(content)\n",
    "            new_documents.append(new_cleaned_text)\n",
    "\n",
    "# Transform the new documents into TF-IDF features\n",
    "new_tfidf = vectorizer.transform(new_documents).toarray()\n",
    "\n",
    "# Predict ESG scores using all models\n",
    "predicted_scores_lasso = model_lasso.predict(new_tfidf)\n",
    "predicted_scores_tree = model_tree.predict(new_tfidf)\n",
    "predicted_scores_xgb = model_xgb.predict(new_tfidf)\n",
    "\n",
    "# Print predictions for each file\n",
    "for idx, (lasso_score, tree_score, xgb_score) in enumerate(zip(predicted_scores_lasso, predicted_scores_tree, predicted_scores_xgb), start=1):\n",
    "    print(f\"File {idx}: Lasso ESG Score: {lasso_score}, Decision Tree ESG Score: {tree_score}, XGBoost ESG Score: {xgb_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross-Validation MSE (per fold): [312.01108056 166.4707854  436.07177085 245.28907707 255.92014289]\n",
      "Linear Regression Average CV MSE: 283.1525713539246\n",
      "Linear Regression Test Set MSE: 78.75377610617883\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lr = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_lr = -cv_scores_lr  # Convert to positive MSE\n",
    "\n",
    "print(\"Linear Regression Cross-Validation MSE (per fold):\", cv_scores_lr)\n",
    "print(\"Linear Regression Average CV MSE:\", np.mean(cv_scores_lr))\n",
    "\n",
    "# Train and evaluate on test set\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(\"Linear Regression Test Set MSE:\", mse_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Cross-Validation MSE (per fold): [ 89.39994493 171.13115684 318.78326313 465.52706559 220.06179173]\n",
      "Lasso Regression Average CV MSE: 252.9806444430663\n",
      "Lasso Regression Test Set MSE: 39.57109092053446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso Regression\n",
    "model_lasso = Lasso(alpha=0.01)  # Adjust alpha for regularization strength\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lasso = cross_val_score(model_lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_lasso = -cv_scores_lasso  # Convert to positive MSE\n",
    "\n",
    "print(\"Lasso Regression Cross-Validation MSE (per fold):\", cv_scores_lasso)\n",
    "print(\"Lasso Regression Average CV MSE:\", np.mean(cv_scores_lasso))\n",
    "\n",
    "# Train and evaluate on test set\n",
    "model_lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = model_lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(\"Lasso Regression Test Set MSE:\", mse_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-Validation MSE (per fold): [678.54044993 259.97185693 442.68954606 918.5174513  329.10486446]\n",
      "Decision Tree Average CV MSE: 525.7648337353579\n",
      "Decision Tree Test Set MSE: 211.3539510200011\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "model_tree = DecisionTreeRegressor(max_depth=10, random_state=42)  # Adjust max_depth as needed\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_tree = cross_val_score(model_tree, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_tree = -cv_scores_tree  # Convert to positive MSE\n",
    "\n",
    "print(\"Decision Tree Cross-Validation MSE (per fold):\", cv_scores_tree)\n",
    "print(\"Decision Tree Average CV MSE:\", np.mean(cv_scores_tree))\n",
    "\n",
    "# Train and evaluate on test set\n",
    "model_tree.fit(X_train, y_train)\n",
    "y_pred_tree = model_tree.predict(X_test)\n",
    "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
    "print(\"Decision Tree Test Set MSE:\", mse_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Cross-Validation MSE (per fold): [388.68854311 153.13135534 429.64131123 435.88815059 220.24391327]\n",
      "XGBoost Average CV MSE: 325.5186547075813\n",
      "XGBoost Test Set MSE: 101.8876814764665\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regressor\n",
    "model_xgb = XGBRegressor(n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_xgb = cross_val_score(model_xgb, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_xgb = -cv_scores_xgb  # Convert to positive MSE\n",
    "\n",
    "print(\"XGBoost Cross-Validation MSE (per fold):\", cv_scores_xgb)\n",
    "print(\"XGBoost Average CV MSE:\", np.mean(cv_scores_xgb))\n",
    "\n",
    "# Train and evaluate on test set\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Test Set MSE:\", mse_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1: Predicted ESG Score (Lasso): 88.68133787127225\n",
      "File 2: Predicted ESG Score (Lasso): 72.00600568683566\n",
      "File 3: Predicted ESG Score (Lasso): 67.22411172526519\n",
      "File 4: Predicted ESG Score (Lasso): 52.9329662211223\n",
      "File 5: Predicted ESG Score (Lasso): 59.567840079223835\n",
      "File 6: Predicted ESG Score (Lasso): 48.24706138005496\n",
      "File 7: Predicted ESG Score (Lasso): 51.57158517704359\n",
      "File 8: Predicted ESG Score (Lasso): 75.51048016775673\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store processed test documents\n",
    "new_documents = []\n",
    "\n",
    "# Preprocess and collect content from each JSON file in the test folder\n",
    "for filename in os.listdir(test_file_path):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(test_file_path, filename), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Extract content (assuming the entire content is relevant)\n",
    "            content = json.dumps(data)  # Convert JSON to string\n",
    "            # Preprocess the content\n",
    "            new_cleaned_text = preprocess_text(content)\n",
    "            new_documents.append(new_cleaned_text)\n",
    "\n",
    "# Transform the new documents using the fitted vectorizer\n",
    "new_tfidf = vectorizer.transform(new_documents).toarray()\n",
    "\n",
    "# Use the trained Lasso model to predict ESG scores for the test files\n",
    "predicted_scores_lasso = model_lasso.predict(new_tfidf)\n",
    "\n",
    "# Print the predicted ESG scores for each test file\n",
    "for idx, score in enumerate(predicted_scores_lasso, start=1):\n",
    "    print(f\"File {idx}: Predicted ESG Score (Lasso): {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
