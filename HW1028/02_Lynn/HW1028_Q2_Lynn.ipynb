{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPT5tff0E2dfFvTTsHXdoQI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Chapter 4. Question 2"],"metadata":{"id":"Drj1DxIFiWTu"}},{"cell_type":"markdown","source":["### It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(μk, σ2) distribution, the Bayes classifer assigns an observation to the class for which the discriminant function is maximized."],"metadata":{"id":"c3NwNkODiaYp"}},{"cell_type":"markdown","source":["## Proof that Maximizing $p_k(x)$ is Equivalent to Maximizing $\\delta_k(x)$\n","\n","Given:\n","$$\n","p_k(x) = \\frac{\\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (x - \\mu_k)^2\\right)}{\\sum_{l=1}^K \\pi_l \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (x - \\mu_l)^2\\right)}\n","$$\n","\n","To classify an observation $x$, we assign it to the class $k$ that maximizes $p_k(x)$.\n","\n","Since the denominator is common for all classes, we can simplify the problem by maximizing the numerator:\n","$$\n","\\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (x - \\mu_k)^2\\right)\n","$$\n","\n","\n","\n","### Step 1: Simplify the Expression\n","Taking the natural logarithm (which preserves the order of maximization):\n","$$\n","\\ln \\left(\\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (x - \\mu_k)^2\\right)\\right)\n","$$\n","\n","Expanding this:\n","$$\n","\\ln(\\pi_k) + \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{1}{2\\sigma^2} (x - \\mu_k)^2\n","$$\n","\n","### Step 2: Remove Constants\n","Since $\\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)$ is constant across all classes, we can ignore it in the maximization:\n","$$\n","\\ln(\\pi_k) - \\frac{1}{2\\sigma^2} (x - \\mu_k)^2\n","$$\n","\n","### Step 3: Expand the Quadratic Term\n","Expanding $-\\frac{1}{2\\sigma^2} (x - \\mu_k)^2$:\n","$$\n","-\\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2)\n","$$\n","\n","Rewriting the discriminant function $\\delta_k(x)$:\n","$$\n","\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\ln(\\pi_k)\n","$$\n","\n","### Thus, we have shown that maximizing $p_k(x)$ is equivalent to maximizing $ \\delta_k(x)$.\n","\n"],"metadata":{"id":"OwQZfMWFsn4D"}}]}