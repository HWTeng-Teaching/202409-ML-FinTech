{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "It was stated in the text that classifying an observation to the class\n",
        "for which (4.17) is largest is equivalent to classifying an observation\n",
        "to the class for which (4.18) is largest. Prove that this is the case. In\n",
        "other words, under the assumption that the observations in the kth\n",
        "class are drawn from a $N(Î¼_k, \\sigma^2)$ distribution, the Bayes classifier\n",
        "assigns an observation to the class for which the discriminant function\n",
        "is maximized."
      ],
      "metadata": {
        "id": "9yliOpU9GqAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bayes classifier assigns an observation to the class for which the posterior probability is maximized. The posterior probability for class k is given by:\n",
        "\n",
        "$$P(Y = k | X = x) = \\frac{P(X = x | Y = k) P(Y = k)}{P(X = x)}$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $P(Y = k | X = x)$ is the posterior probability of class k given the observation x.\n",
        "* $P(X = x | Y = k)$ is the likelihood of observing x given that the true class is k.\n",
        "* $P(Y = k)$ is the prior probability of class k.\n",
        "* $P(X = x)$ is the marginal probability of observing x.\n",
        "\n",
        "Under the assumption that the observations in the kth class are drawn from a $N(\\mu_k, \\sigma^2)$ distribution, the likelihood is given by:\n",
        "\n",
        "$$P(X = x | Y = k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_k)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "Substituting this into the posterior probability equation, we get:\n",
        "\n",
        "$$P(Y = k | X = x) = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_k)^2}{2\\sigma^2}\\right) P(Y = k)}{P(X = x)}$$\n",
        "\n",
        "Since $P(X = x)$ is the same for all classes, we can ignore it when comparing the posterior probabilities. Therefore, we can classify an observation to the class for which the following expression is maximized:\n",
        "\n",
        "$$\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_k)^2}{2\\sigma^2}\\right) P(Y = k)$$\n",
        "\n",
        "Taking the logarithm of this expression, we get:\n",
        "\n",
        "$$-\\frac{(x - \\mu_k)^2}{2\\sigma^2} + \\log(P(Y = k)) + \\text{constant}$$\n",
        "\n",
        "where the constant is $-\\log(\\sqrt{2\\pi\\sigma^2})$, which is the same for all classes.\n",
        "\n",
        "This expression is equivalent to (4.18) in the text. Therefore, classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest.\n",
        "\n",
        "**In other words, under the assumption that the observations in the kth class are drawn from a $N(\\mu_k, \\sigma^2)$ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.**"
      ],
      "metadata": {
        "id": "eNNcluQOXtI3"
      }
    }
  ]
}