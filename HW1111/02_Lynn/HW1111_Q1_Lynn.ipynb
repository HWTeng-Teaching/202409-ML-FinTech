{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 6, Question 3"
      ],
      "metadata": {
        "id": "Su_JNQqOSH-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suppose we estimate the regression coeffcients in a linear regression model by minimizing\n",
        "$$\n",
        "\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p |\\beta_j| \\leq s\n",
        "$$\n",
        "### for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
      ],
      "metadata": {
        "id": "MZBBD7dlSMRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) As we increase s from 0, the training RSS will:\n",
        "### i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
        "### ii. Decrease initially, and then eventually start increasing in a U shape.\n",
        "### iii. Steadily increase.\n",
        "### iv. Steadily decrease.\n",
        "### v. Remain constant."
      ],
      "metadata": {
        "id": "TGIme9veS6UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(iv) Steadily decrease.**\n",
        "\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "The constraint  $ ∑j=1p|βj|≤s $ is an L1 penalty on the regression coefficients.\n",
        "\n",
        "As *s* increases from 0, the constraint becomes less restrictive.  Initially, with a very small *s*, the constraint severely limits the coefficients, forcing many to be zero.  This results in a high training RSS because the model has limited flexibility.\n",
        "\n",
        "As *s* increases, the model is allowed to have larger coefficients (in absolute value), leading to a better fit to the training data and consequently a lower training RSS. **The training RSS will continue to decrease as *s* increases, because the model is given more and more freedom to fit the data.**\n",
        "\n",
        "When *s* is sufficiently large, there's effectively no constraint, and the RSS reaches the minimum possible value attained by ordinary least squares (OLS).  There is no point at which the RSS will increase as we increase the constraint.\n"
      ],
      "metadata": {
        "id": "xNy_yfzuUp2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Repeat (a) for test RSS."
      ],
      "metadata": {
        "id": "5FBpQy7PTEk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(ii) Decrease initially, and then eventually start increasing in a U shape.**\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "As *s* increases from 0, the test RSS will initially decrease.  This is because the model starts with very limited flexibility (due to the small *s*) and is underfitting the data. As *s* increases, the model gains flexibility to capture the true underlying relationship in the data, leading to a decrease in test RSS.\n",
        "\n",
        "However, as *s* continues to increase beyond a certain point, the model starts **overfitting** the training data, which starts fitting the noise in the training set. This overfitting leads to a higher test RSS, because the model's complexity does not generalize well to unseen data.  Thus, the test RSS will decrease initially, then increase in a U shape.\n"
      ],
      "metadata": {
        "id": "jvP_7LHxVq9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Repeat (a) for variance.\n"
      ],
      "metadata": {
        "id": "7m7Zp94-TJy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(iii) Steadily increase.**\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Variance refers to the variability of the model's predictions for different training sets.  \n",
        "\n",
        "As *s* increases from 0, the model gains more flexibility.  With more flexibility, the model is more sensitive to fluctuations in the training data.\n",
        "\n",
        "Different training sets (even with the same underlying relationship) will lead to different fitted models (due to the larger coefficients allowed by higher *s*). This results in a higher variance in the predictions.  Therefore, as *s* increases, the variance of the model steadily increases.\n"
      ],
      "metadata": {
        "id": "Fa6zC5lkWIEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Repeat (a) for (squared) bias.\n"
      ],
      "metadata": {
        "id": "d7Bqn7rmTNch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(ii) Decrease initially, and then eventually start increasing in a U shape.**\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Bias refers to the difference between the average prediction of our model and the true value.\n",
        "\n",
        "As *s* increases from 0, the model's flexibility increases. Initially, with a small *s*, the model is highly constrained and has high bias because it cannot capture the complexity of the true relationship. As *s* increases, the model's complexity increases, allowing it to better approximate the true relationship, thus decreasing the bias.\n",
        "\n",
        "However, as *s* continues to increase beyond a certain point, the model starts **overfitting** the training data.  \n",
        "While it fits the training data extremely well, this overfit model might capture noise in the data, which leads to higher bias in the long term because the model is no longer approximating the true underlying relationship. This leads to the squared bias increasing. Thus the squared bias decreases initially and then increases in a U shape.\n"
      ],
      "metadata": {
        "id": "vIi9BtN2ZMry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Repeat (a) for the irreducible error."
      ],
      "metadata": {
        "id": "P-YosyZ8TQB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(v) Remain constant.**\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Irreducible error is **the error inherent in the data itself** due to factors not captured by the model.\n",
        "\n",
        "It represents the noise in the data, and it is independent of the model's complexity or the choice of *s*.  Changing the value of *s* does not affect the underlying noise in the data. Therefore, the irreducible error remains constant regardless of the value of *s*.\n"
      ],
      "metadata": {
        "id": "i98jGxipZV9j"
      }
    }
  ]
}