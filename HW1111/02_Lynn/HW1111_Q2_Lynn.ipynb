{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 6, Question 4"
      ],
      "metadata": {
        "id": "QWTS8QR2TaJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suppose we estimate the regression coefcients in a linear regression model by minimizing\n",
        "$$\n",
        "\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
        "$$\n",
        "### for a particular value of λ. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
      ],
      "metadata": {
        "id": "ExMghVWCTc8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) As we increase λ from 0, the training RSS will:\n",
        "### i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
        "### ii. Decrease initially, and then eventually start increasing in a U shape.\n",
        "### iii. Steadily increase.\n",
        "### iv. Steadily decrease.\n",
        "### v. Remain constant."
      ],
      "metadata": {
        "id": "m9HX6JSiTraa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(iii) Steadily increase**.\n",
        "\n",
        "**Justificatio**:\n",
        "\n",
        "The given equation represents the objective function for Ridge Regression.  The term $λ∑j=1pβ2j$ is a penalty term on the magnitude of the regression coefficients (βj).  As λ increases, the penalty for large coefficients becomes stronger.  To minimize the objective function, the algorithm will shrink the estimated coefficients towards zero.\n",
        "\n",
        "The training Residual Sum of Squares (RSS) is the first part of the equation: $∑i=1n(yi−β0−∑j=1pβjxij)2$.  \n",
        "**Shrinking the coefficients towards zero generally *increases* the training RSS because the model becomes less flexible and fits the training data less perfectly.** Thus, as λ increases from 0, the training RSS will steadily increase.  \n",
        "\n",
        "The increase in the penalty term is directly offsetting any potential decrease in the residual sum of squares term and forcing a greater overall RSS.\n"
      ],
      "metadata": {
        "id": "H-640VKea8PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Repeat (a) for test RSS."
      ],
      "metadata": {
        "id": "zAznVA5-T3dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(ii) Decrease initially, and then eventually start increasing in a U shape.**\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "As λ increases from 0:\n",
        "\n",
        "1. **Initial Decrease:** Initially, increasing λ will shrink the coefficients, **reducing overfitting**.  A less overfit model generalizes better to unseen data, thus reducing test RSS.\n",
        "\n",
        "2. **Eventual Increase:**  However, increasing λ too much will cause excessive shrinkage, **underfitting** the data.  An underfit model will have high bias, leading to a higher test RSS.\n",
        "\n",
        "Therefore, the test RSS will decrease initially as the model generalizes better, but then increase when the model is overly penalized and starts underfitting. This forms a U-shaped curve.\n"
      ],
      "metadata": {
        "id": "iEr7W7TRbXeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Repeat (a) for variance."
      ],
      "metadata": {
        "id": "ztwLI4q6T61c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(iv) Steadily decrease**.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Variance refers to the variability of the model's predictions.  \n",
        "\n",
        "Ridge regression, by penalizing large coefficients, constrains the model's complexity.  \n",
        "As λ increases, the coefficients are shrunk towards zero, leading to a less complex model.  A less complex model is less sensitive to fluctuations in the training data, which results in lower variance.  \n",
        "\n",
        "Therefore, as λ increases, the variance of the model steadily decreases.\n"
      ],
      "metadata": {
        "id": "E9bKfPftbxSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Repeat (a) for (squared) bias."
      ],
      "metadata": {
        "id": "UpJiRz8aT_fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(iii) Steadily increase**.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Bias refers to the error introduced by approximating a real-life problem, which may be complex, by a simplified model.  \n",
        "\n",
        "Ridge regression, by shrinking coefficients towards zero, simplifies the model.  As λ increases, the model becomes more and more constrained and further from the true function (if one exists).  This increasing simplification leads to higher bias.  Hence, as λ increases, the squared bias of the model will steadily increase.\n"
      ],
      "metadata": {
        "id": "nUUGSgkhb9Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Repeat (a) for the irreducible error."
      ],
      "metadata": {
        "id": "cms623Q7UBPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct answer is **(v) Remain constant**.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "Irreducible error is the error inherent in the data itself due to noise or factors not captured by the model.  It represents the minimum possible prediction error.  Ridge regression (or any other regression technique for that matter) does not affect the underlying data generating process and thus does not impact the irreducible error.  The irreducible error remains constant regardless of the value of λ.\n"
      ],
      "metadata": {
        "id": "_r0t77yecCgP"
      }
    }
  ]
}