{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMt08e1+yxZePb+FAWwoxQO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AOn2pxiKIfoj"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### Q3\n","\n","#### (a) As we increase $s$ from 0, the training RSS (Residual Sum of Squares) will change:\n","Training RSS measures the model's error on the training data. As $s$ increases, the constraint on the regression coefficients, $\\sum_{j=1}^p |\\beta_j| \\leq s$, is relaxed, allowing the model to better fit the training data. Therefore, as $s$ increases, the training RSS steadily decreases.\n","\n","#### (b) As we increase $s$ from 0, the test RSS will change:\n","Test RSS measures the model's error on unseen test data, reflecting the generalization ability of the model. As $s$ increases, initially relaxing the constraint makes the model more flexible and better at capturing key features in the data, causing the test RSS to decrease. However, when $s$ becomes too large, the model starts overfitting the training data, leading to a decrease in generalization ability and an increase in test RSS, resulting in a U-shaped curve.\n","\n","#### (c) As we increase $s$ from 0, the variance of the model will change:\n","Variance reflects how sensitive the model is to fluctuations in the data. As $s$ increases, initially, relaxing the constraint makes the model more flexible, causing the variance to increase as the model becomes more sensitive to the data. However, as $s$ grows further, the variance stabilizes or slightly decreases because the additional flexibility no longer significantly impacts the model's behavior.\n","\n","#### (d) As we increase $s$ from 0, the squared bias of the model will change:\n","Squared bias measures how well the model approximates the true underlying data structure. As $s$ increases, relaxing the constraint makes the model more flexible and better at capturing the true structure of the data. Therefore, the squared bias steadily decreases and approaches zero as $s$ becomes very large and the model becomes unconstrained.\n","\n","#### (e) As we increase $s$ from 0, the irreducible error will change:\n","The irreducible error is caused by random noise in the data and cannot be eliminated even with a perfect model. Since the irreducible error is independent of the model complexity or the value of $s$, it remains constant regardless of changes in $s$.\n"],"metadata":{"id":"lLK878nCJ1An"}},{"cell_type":"code","source":[],"metadata":{"id":"xd0v5yrTJ1SW"},"execution_count":null,"outputs":[]}]}