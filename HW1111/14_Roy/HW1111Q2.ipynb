{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVGfmvuZCk80B67YW31llG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AOn2pxiKIfoj"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### Q4\n","\n","#### (a) As we increase $\\lambda$ from 0, the training RSS will:\n","The training RSS measures the model’s error on the training data. Increasing $\\lambda$ adds a penalty term, $\\lambda \\sum_{j=1}^p \\beta_j^2$, which constrains the coefficients to shrink towards zero. As $\\lambda$ increases:\n","1. At $\\lambda = 0$, the model is unrestricted, and the training RSS is minimized without penalty.\n","2. As $\\lambda$ increases, the penalty term forces the coefficients to shrink, reducing the model's flexibility. This results in a steady increase in the training RSS.\n","\n","#### (b) As we increase $\\lambda$ from 0, the test RSS will:\n","The test RSS measures the model's error on unseen data. As $\\lambda$ increases:\n","1. At $\\lambda = 0$, the model is unrestricted, and overfitting to the training data can occur, leading to high test RSS.\n","2. As $\\lambda$ increases, the penalty reduces overfitting, initially lowering the test RSS by improving generalization.\n","3. If $\\lambda$ becomes too large, the model becomes overly simplistic (underfitting), and the test RSS starts to increase. This results in a U-shaped behavior of the test RSS as $\\lambda$ increases.\n","\n","#### (c) As we increase $\\lambda$ from 0, the variance will:\n","Variance reflects the sensitivity of the model to changes in the training data. As $\\lambda$ increases:\n","1. Initially, variance is high because an unrestricted model ($\\lambda = 0$) can overfit the training data.\n","2. As $\\lambda$ increases, the penalty term reduces the model’s flexibility, leading to a steady decrease in variance.\n","\n","#### (d) As we increase $\\lambda$ from 0, the squared bias will:\n","Squared bias measures how closely the model approximates the true underlying data structure. As $\\lambda$ increases:\n","1. At $\\lambda = 0$, the model has no penalty and can closely fit the training data, resulting in low bias.\n","2. As $\\lambda$ increases, the penalty term constrains the coefficients, reducing the model's ability to capture the true underlying structure, causing the squared bias to increase steadily.\n","\n","#### (e) As we increase $\\lambda$ from 0, the irreducible error will:\n","The irreducible error is caused by random noise in the data, which cannot be reduced by the model. Since the irreducible error is independent of $\\lambda$, it remains constant regardless of changes in $\\lambda$.\n"],"metadata":{"id":"lLK878nCJ1An"}},{"cell_type":"code","source":[],"metadata":{"id":"xd0v5yrTJ1SW"},"execution_count":null,"outputs":[]}]}