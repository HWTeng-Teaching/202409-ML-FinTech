{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we estimate the regression coefficients in a linear regression\n",
        "model by minimizing\n",
        "\n",
        "for a particular value of s. For parts (a) through (e), indicate which\n",
        "of i. through v. is correct. Justify your answer."
      ],
      "metadata": {
        "id": "OKksGigBaL7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) As we increase s from 0, the training RSS will:\n",
        "\n",
        "i. Increase initially, and then eventually start decreasing in an\n",
        "inverted U shape.\n",
        "\n",
        "ii. Decrease initially, and then eventually start increasing in a\n",
        "U shape.\n",
        "\n",
        "iii. Steadily increase.\n",
        "\n",
        "iv. Steadily decrease.\n",
        "\n",
        "v. Remain constant."
      ],
      "metadata": {
        "id": "_jJQX5_EaagX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer: **iv. Steadily decrease**\n",
        "\n",
        "When\n",
        "s=0: All coefficients (\n",
        "β\n",
        "j\n",
        "​\n",
        " ) are forced to be 0 due to the constraint. This gives the simplest model, and the training RSS is likely to be high.\n",
        "\n",
        "As s increases: The constraint loosens, allowing larger values of\n",
        "β\n",
        "j\n",
        "​. This provides more flexibility to fit the data, and the training RSS decreases as the model becomes less restricted and better fits the data.\n",
        "\n",
        "When s is very large: The constraint becomes non-restrictive, effectively allowing the model to behave like standard least squares regression. The training RSS reaches its minimum value, corresponding to the least squares solution.\n"
      ],
      "metadata": {
        "id": "6wnPc5eIadBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P9GlpPtlbVvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Repeat (a) for test RSS.\n",
        "\n",
        "answer: **ii. Decrease initially, and then eventually start increasing in a U shape.**\n",
        "\n",
        "\n",
        "When s=0: All coefficients (\n",
        "β\n",
        "j\n",
        "​\n",
        " =0), leading to a constant model (predicting only the intercept). This results in high bias and poor generalization, so the test RSS is likely to be high.\n",
        "\n",
        "As s increases: Allowing nonzero coefficients reduces bias and improves the model's flexibility, which tends to improve the fit on both training and test data. The test RSS decreases initially.\n",
        "\n",
        "When s is very large: The model becomes highly flexible (approaching ordinary least squares regression), which risks overfitting the training data. This overfitting leads to increased variance, and the test RSS starts to increase."
      ],
      "metadata": {
        "id": "vOv51by8bHED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Repeat (a) for variance.\n",
        "\n",
        "answer :**iii. Steadily increase.**\n",
        "\n",
        "When\n",
        "s=0: All coefficients (\n",
        "β\n",
        "j\n",
        "​\n",
        " =0), meaning the model is extremely rigid (only predicts the intercept). In this case, the variance is low, as the predictions are not affected much by changes in the training data.\n",
        "\n",
        "As s increases: The constraint loosens, allowing the model to become more flexible and fit the training data more closely. With increased flexibility, the model becomes more sensitive to variations in the training data, so the variance increases.\n",
        "\n",
        "When s is very large: The model approaches ordinary least squares regression, which is highly flexible and thus highly sensitive to changes in the training data. Variance continues to increase steadily."
      ],
      "metadata": {
        "id": "kQFvbnH0bxov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Repeat (a) for (squared) bias.\n",
        "\n",
        "answer is: **iv. Steadily decrease.**\n",
        "\n",
        "When s = 0: All coefficients (\\(\\beta_j = 0), so the model predicts only the intercept. This is a highly restrictive and simple model, resulting in high bias because it cannot capture the complexity of the data.\n",
        "\n",
        "As s increases: The model becomes less restrictive, allowing nonzero coefficients and greater flexibility to fit the data. As a result, the bias decreases, as the model better approximates the true relationship in the data.\n",
        "\n",
        "When s is very large: The constraint becomes non-restrictive, and the model approaches ordinary least squares regression. At this point, the model has minimal bias, as it can fully adapt to the training data."
      ],
      "metadata": {
        "id": "CDaCKkTBb1NX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Repeat (a) for the irreducible error.\n",
        "\n",
        "**answer : v. Remain constant.**\n",
        "\n",
        "The irreducible error is independent of the model or the value of s. It represents the part of the error that cannot be reduced, no matter how flexible or restrictive the model is.\n",
        "\n",
        "Changing s does not impact this component of the error.\n"
      ],
      "metadata": {
        "id": "FZ22xw4WaKju"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-SRafL_cJGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}