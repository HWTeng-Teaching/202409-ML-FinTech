{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch06_Q4\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing:\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}\\beta_{j}x_{ij}\\right)^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n",
    "$\n",
    "\n",
    "for a particular value of \\( $\\lambda $\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "**Available Patterns:**\n",
    "- i. Inverted U-shape: increases then decreases  \n",
    "- ii. U-shape: decreases then increases  \n",
    "- iii. Steadily increases  \n",
    "- iv. Steadily decreases  \n",
    "- v. Remains constant  \n",
    "\n",
    "## (a) As we increase \\( $\\lambda $\\) from 0, the training RSS will:\n",
    "**Answer**: iii. Steadily increases\n",
    "\n",
    "In Ridge regression, as \\( $\\lambda $\\) increases, the penalty on the coefficients increases, causing the coefficients to shrink towards zero. This reduces the model's flexibility, which results in the training RSS increasing steadily because the model is less able to fit the training data well.\n",
    "\n",
    "## (b) Repeat (a) for test RSS.\n",
    "**Answer**: ii. U-shape: decreases then increases\n",
    "\n",
    "As \\( $\\lambda $\\) increases, the model initially becomes less prone to overfitting, and the test RSS decreases. However, when \\( $\\lambda $\\) becomes too large, the model underfits the data, and the test RSS begins to increase again, forming a U-shape.\n",
    "\n",
    "## (c) Repeat (a) for variance.\n",
    "**Answer**: iv. Steadily decreases\n",
    "\n",
    "Larger \\( $\\lambda $\\) values shrink the coefficients, making the model less sensitive to fluctuations in the training data. This results in a more stable model with lower variance, as the model's predictions become less variable.\n",
    "\n",
    "## (d) Repeat (a) for (squared) bias.\n",
    "**Answer**: iii. Steadily increases\n",
    "\n",
    "As \\( $\\lambda $\\) increases, the model becomes more rigid and less able to fit the training data well, which results in an increase in bias. This happens because the model has less flexibility to approximate the true data distribution.\n",
    "\n",
    "## (e) Repeat (a) for the irreducible error.\n",
    "**Answer**: v. Remains constant\n",
    "\n",
    "The irreducible error is the noise in the data that no model can reduce. Changes in \\( $\\lambda $\\) affect the model's flexibility and fit, but they do not impact the irreducible error, which remains constant regardless of \\( $\\lambda $\\).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
