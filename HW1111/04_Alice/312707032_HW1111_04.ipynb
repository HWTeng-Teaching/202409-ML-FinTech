{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.\n",
    "In this exercise, we will predict the number of applications received using the other variables in the College data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    " Split the data set into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the College dataset\n",
    "data = pd.read_csv('/Users/chenpinyu/anaconda3/lib/python3.11/site-packages/ISLP/data/College.csv')\n",
    "\n",
    "# Convert 'Private' column to binary (1 for 'Yes', 0 for 'No')\n",
    "data['Private'] = data['Private'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Ensure all non-numeric columns are dropped or converted\n",
    "X = data.drop(['Apps'], axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = data['Apps']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "  Fit a linear model using least squares on the training set, and report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Test Error (MSE): 1931803.1942069773\n"
     ]
    }
   ],
   "source": [
    "# (b) Fit a linear model using least squares on the training set\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "lin_mse = mean_squared_error(y_test, y_pred_lin)\n",
    "print(f'Linear Model Test Error (MSE): {lin_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    " Fit a ridge regression model on the training set, with chosen by cross-validation. Report the test error obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Test Error (MSE): 1926694.7197764048\n"
     ]
    }
   ],
   "source": [
    "# (c) Fit a ridge regression model on the training set with lambda chosen by cross-validation\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, 13), store_cv_values=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f'Ridge Regression Test Error (MSE): {ridge_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    " Fit a lasso model on the training set, with chosen by cross validation. Report the test error obtained, along with the num ber of non-zero coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Test Error (MSE): 2250488.9463233114\n",
      "Number of non-zero coefficients in Lasso: 7\n"
     ]
    }
   ],
   "source": [
    "# (d) Fit a lasso model on the training set with lambda chosen by cross-validation\n",
    "lasso = LassoCV(cv=10, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "non_zero_coef = np.sum(lasso.coef_ != 0)\n",
    "print(f'Lasso Regression Test Error (MSE): {lasso_mse}')\n",
    "print(f'Number of non-zero coefficients in Lasso: {non_zero_coef}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "  Fit a PCR model on the training set, with M chosen by cross validation. Report the test error obtained, along with the value of M selected by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of components for PCR: 17\n",
      "PCR Test Error (MSE): 1931803.194207\n"
     ]
    }
   ],
   "source": [
    "# (e) Fit a PCR model on the training set, with M chosen by cross-validation\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Cross-validation to choose the best number of components\n",
    "mse_list = []\n",
    "for m in range(1, X_train_pca.shape[1] + 1):\n",
    "    lin_reg_pca = LinearRegression()\n",
    "    mse = -np.mean(cross_val_score(lin_reg_pca, X_train_pca[:, :m], y_train, cv=10, scoring='neg_mean_squared_error'))\n",
    "    mse_list.append(mse)\n",
    "\n",
    "best_m_pcr = np.argmin(mse_list) + 1\n",
    "print(f'Best number of components for PCR: {best_m_pcr}')\n",
    "\n",
    "# Fit PCR model with the best number of components\n",
    "lin_reg_pca = LinearRegression()\n",
    "lin_reg_pca.fit(X_train_pca[:, :best_m_pcr], y_train)\n",
    "y_pred_pcr = lin_reg_pca.predict(X_test_pca[:, :best_m_pcr])\n",
    "pcr_mse = mean_squared_error(y_test, y_pred_pcr)\n",
    "print(f'PCR Test Error (MSE): {pcr_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    " Fit a PLS model on the training set, with M chosen by cross validation. Report the test error obtained, along with the value of M selected by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of components for PLS: 14\n",
      "PLS Test Error (MSE): 1930317.7198832259\n"
     ]
    }
   ],
   "source": [
    "# (f) Fit a PLS model on the training set, with M chosen by cross-validation\n",
    "mse_list_pls = []\n",
    "for m in range(1, X_train.shape[1] + 1):\n",
    "    pls = PLSRegression(n_components=m)\n",
    "    mse = -np.mean(cross_val_score(pls, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n",
    "    mse_list_pls.append(mse)\n",
    "\n",
    "best_m_pls = np.argmin(mse_list_pls) + 1\n",
    "print(f'Best number of components for PLS: {best_m_pls}')\n",
    "\n",
    "# Fit PLS model with the best number of components\n",
    "pls = PLSRegression(n_components=best_m_pls)\n",
    "pls.fit(X_train, y_train)\n",
    "y_pred_pls = pls.predict(X_test)\n",
    "pls_mse = mean_squared_error(y_test, y_pred_pls)\n",
    "print(f'PLS Test Error (MSE): {pls_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    " Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five ap proaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The models' test errors (MSE) are close, suggesting similar predictive accuracy.\n",
    "\n",
    "Ridge Regression performed slightly better than Linear Regression, highlighting the benefit of regularization in reducing overfitting.\n",
    "\n",
    "Lasso Regression showed higher test error, likely due to excessive regularization removing key predictors.\n",
    "\n",
    "PCR's performance was on par with Ridge and Linear Regression, with 17 components explaining much of the dataâ€™s variability.\n",
    "\n",
    "PLS, with 2 components, slightly lagged behind PCR and Ridge, indicating fewer components might not capture enough data variability.\n",
    "\n",
    "Overall, Ridge Regression offered the best compromise between model complexity and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
