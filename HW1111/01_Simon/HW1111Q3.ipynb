{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "8. In this exercise, we will generate simulated data, and will then use\n",
        "this data to perform forward and backward stepwise selection."
      ],
      "metadata": {
        "id": "opfyW-YvSN65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Create a random number generator and use its normal() method\n",
        "to generate a predictor X of length n = 100, as well as a noise\n",
        "vector ϵ of length n = 100.\n"
      ],
      "metadata": {
        "id": "iGq87j0RSRXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate predictor X and noise epsilon\n",
        "n = 100\n",
        "X = np.random.normal(loc=0, scale=1, size=n)\n",
        "epsilon = np.random.normal(loc=0, scale=1, size=n)\n",
        "\n",
        "print(\"Predictor X:\", X)\n",
        "print(\"Noise epsilon:\", epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltsj_ZnNSorw",
        "outputId": "1252b0b9-4043-460b-d965-9deaa0c0c3e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictor X: [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
            "  1.57921282  0.76743473 -0.46947439  0.54256004 -0.46341769 -0.46572975\n",
            "  0.24196227 -1.91328024 -1.72491783 -0.56228753 -1.01283112  0.31424733\n",
            " -0.90802408 -1.4123037   1.46564877 -0.2257763   0.0675282  -1.42474819\n",
            " -0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
            " -0.60170661  1.85227818 -0.01349722 -1.05771093  0.82254491 -1.22084365\n",
            "  0.2088636  -1.95967012 -1.32818605  0.19686124  0.73846658  0.17136828\n",
            " -0.11564828 -0.3011037  -1.47852199 -0.71984421 -0.46063877  1.05712223\n",
            "  0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
            "  1.03099952  0.93128012 -0.83921752 -0.30921238  0.33126343  0.97554513\n",
            " -0.47917424 -0.18565898 -1.10633497 -1.19620662  0.81252582  1.35624003\n",
            " -0.07201012  1.0035329   0.36163603 -0.64511975  0.36139561  1.53803657\n",
            " -0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
            "  0.09176078 -1.98756891 -0.21967189  0.35711257  1.47789404 -0.51827022\n",
            " -0.8084936  -0.50175704  0.91540212  0.32875111 -0.5297602   0.51326743\n",
            "  0.09707755  0.96864499 -0.70205309 -0.32766215 -0.39210815 -1.46351495\n",
            "  0.29612028  0.26105527  0.00511346 -0.23458713]\n",
            "Noise epsilon: [-1.41537074 -0.42064532 -0.34271452 -0.80227727 -0.16128571  0.40405086\n",
            "  1.8861859   0.17457781  0.25755039 -0.07444592 -1.91877122 -0.02651388\n",
            "  0.06023021  2.46324211 -0.19236096  0.30154734 -0.03471177 -1.16867804\n",
            "  1.14282281  0.75193303  0.79103195 -0.90938745  1.40279431 -1.40185106\n",
            "  0.58685709  2.19045563 -0.99053633 -0.56629773  0.09965137 -0.50347565\n",
            " -1.55066343  0.06856297 -1.06230371  0.47359243 -0.91942423  1.54993441\n",
            " -0.78325329 -0.32206152  0.81351722 -1.23086432  0.22745993  1.30714275\n",
            " -1.60748323  0.18463386  0.25988279  0.78182287 -1.23695071 -1.32045661\n",
            "  0.52194157  0.29698467  0.25049285  0.34644821 -0.68002472  0.2322537\n",
            "  0.29307247 -0.71435142  1.86577451  0.47383292 -1.1913035   0.65655361\n",
            " -0.97468167  0.7870846   1.15859558 -0.82068232  0.96337613  0.41278093\n",
            "  0.82206016  1.89679298 -0.24538812 -0.75373616 -0.88951443 -0.81581028\n",
            " -0.07710171  0.34115197  0.2766908   0.82718325  0.01300189  1.45353408\n",
            " -0.26465683  2.72016917  0.62566735 -0.85715756 -1.0708925   0.48247242\n",
            " -0.22346279  0.71400049  0.47323762 -0.07282891 -0.84679372 -1.51484722\n",
            " -0.44651495  0.85639879  0.21409374 -1.24573878  0.17318093  0.38531738\n",
            " -0.88385744  0.15372511  0.05820872 -1.1429703 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Generate a response vector Y of length n = 100 according to\n",
        "the model\n",
        "\n",
        "$Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_3X^3+\\epsilon$\n",
        "\n",
        "where $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ are constants of your choice."
      ],
      "metadata": {
        "id": "2Kbga_flS0L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the coefficients\n",
        "beta_0 = 1\n",
        "beta_1 = 2\n",
        "beta_2 = -1\n",
        "beta_3 = 0.5\n",
        "\n",
        "# Generate the response vector Y\n",
        "Y = beta_0 + beta_1 * X + beta_2 * (X ** 2) + beta_3 * (X ** 3) + epsilon\n",
        "\n",
        "print(\"Response Y:\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLF8MmGpTYzx",
        "outputId": "868e6145-528b-4699-e5e0-c8a2c13f619c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Y: [  0.3926085    0.28238746   1.66901493   2.69058771   0.30916068\n",
            "   0.87453912   5.51990819   2.34648388   0.04645789   1.79615985\n",
            "  -2.11012341  -0.22538696   1.49269194  -7.52587601  -8.18364799\n",
            "  -0.22808344  -2.60569558   0.37658142  -0.87206949  -4.47576783\n",
            "   4.14840251  -0.41766946   2.53344463  -6.72730827   0.12107449\n",
            "   3.40067937  -4.37971991   1.07046401  -0.57073811  -0.18435781\n",
            "  -2.2250517    4.51970741  -0.08948157  -2.35224016   1.32734423\n",
            "  -2.29202095   0.59540563 -10.8445762   -3.77844508   0.12791842\n",
            "   2.36041526   2.62302852  -0.85292769   0.47811348  -5.49923316\n",
            "  -0.36254411  -1.41928736   1.26695139   2.11139073  -8.07744442\n",
            "   1.81064971   0.39944368  -1.64718287   2.19588709   2.84006714\n",
            "   1.68476771   0.18752882   0.74501362   0.37966358   3.12016301\n",
            "  -1.2176492    1.37809763  -1.95511577  -4.49983804   3.19644361\n",
            "   3.53319912   1.67266776   4.40209859   1.37075081  -1.59439798\n",
            "   0.72627036   2.71386255   0.84993972   3.93753923 -19.81560351\n",
            "   3.07307185   1.17984862   1.75274755   0.91083099  -8.13127498\n",
            "   1.13276762   0.75230937   2.3147113    0.10772323  -1.75835263\n",
            "   0.39556507   2.84961648   1.49436128  -1.26129751   0.31585266\n",
            "   0.73867353   3.30984244  -0.85590444  -1.02601486   0.20507274\n",
            "  -5.25092229   0.63367888   1.61658123   1.06840955  -0.67363048]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Use forward stepwise selection in order to select a model containing\n",
        "the predictors $X$, $X^2$, . . . , $X^{10}$. What is the model obtained\n",
        "according to Cp? Report the coefficients of the model obtained."
      ],
      "metadata": {
        "id": "juDV1fCxTtlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as sm\n",
        "\n",
        "# Create a DataFrame with the predictors and response\n",
        "df = pd.DataFrame({'Y': Y})\n",
        "for i in range(1, 11):\n",
        "    df[f'X{i}'] = X ** i\n",
        "\n",
        "# Initialize an empty list to store the models and their Cp values\n",
        "models = []\n",
        "cps = []\n",
        "\n",
        "# Perform forward stepwise selection\n",
        "current_predictors = [] # for k=0, ..., p-1\n",
        "for i in range(10):\n",
        "    best_r2 = float('-inf') # initialize to negative infinity\n",
        "    best_predictor = None\n",
        "    for predictor in [f'X{j}' for j in range(1, 11) if f'X{j}' not in current_predictors]:\n",
        "        formula = 'Y ~ ' + ' + '.join(current_predictors + [predictor])\n",
        "        model = sm.ols(formula, data=df).fit()\n",
        "        cp = model.aic\n",
        "        r2 = model.rsquared\n",
        "        if r2 > best_r2: # best predictor: update according to highest R-squared value\n",
        "            best_r2 = r2\n",
        "            best_cp = cp\n",
        "            best_predictor = predictor\n",
        "    if best_predictor is not None:\n",
        "        current_predictors.append(best_predictor)\n",
        "        models.append(sm.ols('Y ~ ' + ' + '.join(current_predictors), data=df).fit())\n",
        "        cps.append(best_cp)\n",
        "\n",
        "# Find the model with the smallest Cp\n",
        "best_model_index = np.argmin(cps)\n",
        "best_model = models[best_model_index]\n",
        "\n",
        "# Report the coefficients of the selected model\n",
        "print(\"Model obtained according to Cp:\")\n",
        "print(best_model.summary())\n",
        "print(\"\\nCoefficients:\")\n",
        "best_model.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "Xc2GN3QyUV70",
        "outputId": "d52d4bae-7b13-403e-8237-69302937b9c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model obtained according to Cp:\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.935\n",
            "Model:                            OLS   Adj. R-squared:                  0.932\n",
            "Method:                 Least Squares   F-statistic:                     341.5\n",
            "Date:                Sat, 16 Nov 2024   Prob (F-statistic):           1.91e-55\n",
            "Time:                        05:13:34   Log-Likelihood:                -132.24\n",
            "No. Observations:                 100   AIC:                             274.5\n",
            "Df Residuals:                      95   BIC:                             287.5\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      0.8316      0.121      6.878      0.000       0.592       1.072\n",
            "X3             0.4282      0.101      4.231      0.000       0.227       0.629\n",
            "X1             2.0371      0.223      9.145      0.000       1.595       2.479\n",
            "X2            -0.7626      0.104     -7.325      0.000      -0.969      -0.556\n",
            "X10           -0.0002      0.000     -1.391      0.168      -0.000    6.53e-05\n",
            "==============================================================================\n",
            "Omnibus:                        1.786   Durbin-Watson:                   2.260\n",
            "Prob(Omnibus):                  0.409   Jarque-Bera (JB):                1.541\n",
            "Skew:                           0.161   Prob(JB):                        0.463\n",
            "Kurtosis:                       2.485   Cond. No.                     3.97e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 3.97e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Intercept    0.831634\n",
              "X3           0.428210\n",
              "X1           2.037134\n",
              "X2          -0.762590\n",
              "X10         -0.000153\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Intercept</th>\n",
              "      <td>0.831634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.428210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>2.037134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X2</th>\n",
              "      <td>-0.762590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>-0.000153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Repeat (c), using backwards stepwise selection. How does your\n",
        "answer compare to the results in (c)?"
      ],
      "metadata": {
        "id": "Zb8A8nP9ZULo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform backward stepwise selection\n",
        "current_predictors = [f'X{i}' for i in range(1, 11)]\n",
        "models = []\n",
        "cps = []\n",
        "\n",
        "# Perform backward stepwise selection\n",
        "current_predictors = [f'X{i}' for i in range(1, 11)]\n",
        "models = []\n",
        "cps = []\n",
        "\n",
        "while len(current_predictors) > 0:\n",
        "    worst_cp = float('inf')\n",
        "    worst_predictor = None\n",
        "    best_model_temp = None  # Initialize best_model_temp\n",
        "\n",
        "    for predictor in current_predictors:\n",
        "        temp_predictors = current_predictors[:]\n",
        "        temp_predictors.remove(predictor)\n",
        "\n",
        "        # Check if temp_predictors is empty, if yes, break the loop\n",
        "        if not temp_predictors:\n",
        "            break\n",
        "\n",
        "        formula = 'Y ~ ' + ' + '.join(temp_predictors)\n",
        "        model = sm.ols(formula, data=df).fit()\n",
        "        cp = model.aic\n",
        "\n",
        "        if cp < worst_cp:\n",
        "            worst_cp = cp\n",
        "            worst_predictor = predictor\n",
        "            best_model_temp = model\n",
        "\n",
        "    if worst_predictor is not None:\n",
        "        current_predictors.remove(worst_predictor)\n",
        "        models.append(best_model_temp)\n",
        "        cps.append(worst_cp)\n",
        "    else:\n",
        "        # If worst_predictor is None (all predictors removed), break\n",
        "        break\n",
        "\n",
        "# Find the model with the smallest Cp\n",
        "best_model_index_backward = np.argmin(cps)\n",
        "best_model_backward = models[best_model_index_backward]\n",
        "\n",
        "\n",
        "# Report the coefficients of the selected model\n",
        "print(\"Model obtained according to Cp (backward stepwise selection):\")\n",
        "print(best_model_backward.summary())\n",
        "print(\"\\nCoefficients:\")\n",
        "best_model_backward.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "da5b49sOaKWP",
        "outputId": "d85d147a-c72c-4d42-e1d1-62765e770d35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model obtained according to Cp (backward stepwise selection):\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.937\n",
            "Model:                            OLS   Adj. R-squared:                  0.934\n",
            "Method:                 Least Squares   F-statistic:                     279.4\n",
            "Date:                Sat, 16 Nov 2024   Prob (F-statistic):           8.74e-55\n",
            "Time:                        05:33:06   Log-Likelihood:                -130.68\n",
            "No. Observations:                 100   AIC:                             273.4\n",
            "Df Residuals:                      94   BIC:                             289.0\n",
            "Df Model:                           5                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      0.6725      0.107      6.270      0.000       0.460       0.885\n",
            "X1             1.9422      0.228      8.518      0.000       1.489       2.395\n",
            "X3             0.5086      0.111      4.585      0.000       0.288       0.729\n",
            "X6            -0.4893      0.098     -4.992      0.000      -0.684      -0.295\n",
            "X8             0.1810      0.043      4.210      0.000       0.096       0.266\n",
            "X10           -0.0164      0.004     -3.925      0.000      -0.025      -0.008\n",
            "==============================================================================\n",
            "Omnibus:                        0.330   Durbin-Watson:                   2.167\n",
            "Prob(Omnibus):                  0.848   Jarque-Bera (JB):                0.477\n",
            "Skew:                           0.113   Prob(JB):                        0.788\n",
            "Kurtosis:                       2.748   Cond. No.                     4.19e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 4.19e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "Coefficients:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Intercept    0.672485\n",
              "X1           1.942241\n",
              "X3           0.508550\n",
              "X6          -0.489339\n",
              "X8           0.180994\n",
              "X10         -0.016394\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Intercept</th>\n",
              "      <td>0.672485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X1</th>\n",
              "      <td>1.942241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X3</th>\n",
              "      <td>0.508550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X6</th>\n",
              "      <td>-0.489339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X8</th>\n",
              "      <td>0.180994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X10</th>\n",
              "      <td>-0.016394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Now fit a lasso model to the simulated data, again using $X$, $X^2$, . . . , $X^{10}$ as predictors. Use cross-validation to select the optimal\n",
        "value of λ. Create plots of the cross-validation error as a function\n",
        "of λ. Report the resulting coefficient estimates, and discuss the\n",
        "results obtained."
      ],
      "metadata": {
        "id": "Nwu2_AI2fB0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Now fit a lasso model to the simulated data, again using  X ,  X^2 , . . . ,  X^10  as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained.\n",
        "\n",
        "from sklearn.linear_model import LassoCV,Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the predictor matrix X\n",
        "X = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']]\n",
        "y = df['Y']\n",
        "\n",
        "# Perform Lasso Regression with cross-validation to find the optimal lambda\n",
        "lasso_cv = LassoCV(cv=5, random_state=42)  # Use 5-fold cross-validation\n",
        "lasso_cv.fit(X, y)\n",
        "\n",
        "# Get the optimal lambda value\n",
        "optimal_lambda = lasso_cv.alpha_\n",
        "\n",
        "# Fit a Lasso model using the optimal lambda\n",
        "lasso = Lasso(alpha=optimal_lambda)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Report the coefficient estimates\n",
        "print(\"Coefficient estimates:\")\n",
        "print(pd.DataFrame({'Predictor': X.columns, 'Coefficient': lasso.coef_}))\n",
        "\n",
        "# Plot the cross-validation error as a function of lambda\n",
        "plt.plot(lasso_cv.alphas_, lasso_cv.mse_path_.mean(axis=1))\n",
        "plt.xlabel(\"Lambda\")\n",
        "plt.ylabel(\"Cross-Validation Error (MSE)\")\n",
        "plt.title(\"Lasso Cross-Validation Error\")\n",
        "plt.show()\n",
        "\n",
        "# Discussion of the results:\n",
        "# The Lasso model can be used to perform variable selection and regularization.\n",
        "# In this case, the cross-validation procedure selects an optimal value of lambda\n",
        "# that balances the model's fit and complexity. The coefficient estimates will\n",
        "# highlight which predictors are deemed most important by the model.\n",
        "# The plot of the cross-validation error can help in understanding how\n",
        "# the model's error changes as the regularization parameter lambda is varied."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Svz3n3ckfzfN",
        "outputId": "9d3f2aee-c84a-4f75-c43a-15eb222d9c5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13050972300982266, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15282768946190117, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1696993079956428, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18194038001831814, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19265984501339517, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22702052314110688, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7680583125139719, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1892824870034246, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20016118375463066, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6958382739211402, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9373625668575869, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9387857359689633, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7297800521076283, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3476313387902792, tolerance: 0.10615442484824691\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1959029333427793, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2686475751405055, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31481866583135343, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.34755054215276004, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37108103246976043, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3873605489674219, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3977000654319909, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.40314720661950787, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.40458133662627915, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.40274839684059316, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39828163771414893, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.391717738462944, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.40216897792998907, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5462422895762984, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6124736856215804, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6507926575134206, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6752569586943764, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6896697459993675, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6959808980867592, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6956855292352202, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6902384134161821, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6968608894617887, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7667515093445445, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.092377187079137, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.922965018102218, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.670567300980224, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2612790093739932, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7585406332211164, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1539481276957986, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4470101350724462, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6456112408985177, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7560570635074555, tolerance: 0.10000450905666489\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2451469259762007, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9576631106286868, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.340833591700175, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9033477843665878, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1307059736469682, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.059966073184114, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7454247933761735, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2464098018061236, tolerance: 0.1114637365954842\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3410459493891267, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7256584069033352, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.184628364703599, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2987908318449968, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1118344342075943, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6809289216049592, tolerance: 0.11949315314798992\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient estimates:\n",
            "  Predictor  Coefficient\n",
            "0        X1     0.000000\n",
            "1        X2    -0.000000\n",
            "2        X3     0.000000\n",
            "3        X4    -0.000000\n",
            "4        X5     0.000000\n",
            "5        X6    -0.000000\n",
            "6        X7     0.000000\n",
            "7        X8    -0.000000\n",
            "8        X9     0.000000\n",
            "9       X10    -0.001318\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Now generate a response vector Y according to the model\n",
        "$Y = \\beta_0 + \\beta_7X^7 + \\epsilon$,\n",
        "and perform forward stepwise selection and the lasso. Discuss\n",
        "the results obtained."
      ],
      "metadata": {
        "id": "d6V5su8RgWQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the coefficients\n",
        "beta_0 = 1\n",
        "beta_7 = 2\n",
        "\n",
        "# Generate the response vector Y\n",
        "Y = beta_0 + beta_7 * df['X7'] + epsilon\n",
        "\n",
        "# Update the DataFrame with the new response vector\n",
        "df['Y'] = Y\n",
        "\n",
        "# Perform forward stepwise selection (same as in part (c), but with the new Y)\n",
        "current_predictors = []\n",
        "models = []\n",
        "cps = []\n",
        "\n",
        "for i in range(10):\n",
        "    best_r2 = float('-inf')\n",
        "    best_predictor = None\n",
        "    for predictor in [f'X{j}' for j in range(1, 11) if f'X{j}' not in current_predictors]:\n",
        "        formula = 'Y ~ ' + ' + '.join(current_predictors + [predictor])\n",
        "        model = sm.ols(formula, data=df).fit()\n",
        "        cp = model.aic\n",
        "        r2 = model.rsquared\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_cp = cp\n",
        "            best_predictor = predictor\n",
        "    if best_predictor is not None:\n",
        "        current_predictors.append(best_predictor)\n",
        "        models.append(sm.ols('Y ~ ' + ' + '.join(current_predictors), data=df).fit())\n",
        "        cps.append(best_cp)\n",
        "\n",
        "best_model_index_forward_f = np.argmin(cps)\n",
        "best_model_forward_f = models[best_model_index_forward_f]\n",
        "\n",
        "print(\"Model obtained according to Cp (forward stepwise selection - part f):\")\n",
        "print(best_model_forward_f.summary())\n",
        "\n",
        "# Perform Lasso Regression with cross-validation (same as in part (e), but with the new Y)\n",
        "X = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']]\n",
        "y = df['Y']\n",
        "\n",
        "lasso_cv = LassoCV(cv=5, random_state=42)\n",
        "lasso_cv.fit(X, y)\n",
        "\n",
        "optimal_lambda = lasso_cv.alpha_\n",
        "lasso = Lasso(alpha=optimal_lambda)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "print(\"Coefficient estimates (Lasso - part f):\")\n",
        "print(pd.DataFrame({'Predictor': X.columns, 'Coefficient': lasso.coef_}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8ipevRegWFb",
        "outputId": "08d22f00-b283-4b7a-c068-f2ef51c1d607"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model obtained according to Cp (forward stepwise selection - part f):\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       1.000\n",
            "Model:                            OLS   Adj. R-squared:                  1.000\n",
            "Method:                 Least Squares   F-statistic:                 1.178e+06\n",
            "Date:                Sat, 16 Nov 2024   Prob (F-statistic):          5.44e-219\n",
            "Time:                        05:49:30   Log-Likelihood:                -131.90\n",
            "No. Observations:                 100   AIC:                             271.8\n",
            "Df Residuals:                      96   BIC:                             282.2\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      0.8434      0.119      7.063      0.000       0.606       1.080\n",
            "X7             1.9859      0.009    214.680      0.000       1.968       2.004\n",
            "X2             0.2084      0.104      2.013      0.047       0.003       0.414\n",
            "X9             0.0022      0.001      1.647      0.103      -0.000       0.005\n",
            "==============================================================================\n",
            "Omnibus:                        1.722   Durbin-Watson:                   2.263\n",
            "Prob(Omnibus):                  0.423   Jarque-Bera (JB):                1.454\n",
            "Skew:                           0.137   Prob(JB):                        0.483\n",
            "Kurtosis:                       2.476   Cond. No.                         917.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "Coefficient estimates (Lasso - part f):\n",
            "  Predictor  Coefficient\n",
            "0        X1     0.000000\n",
            "1        X2    -0.000000\n",
            "2        X3     0.000000\n",
            "3        X4    -0.000000\n",
            "4        X5     0.000000\n",
            "5        X6    -0.000000\n",
            "6        X7     0.000000\n",
            "7        X8    -0.000000\n",
            "8        X9     0.261186\n",
            "9       X10    -0.011639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion of the Results:\n",
        "\n",
        "#### **OLS Regression Results (Forward Stepwise Selection)**\n",
        "1. **Model Overview**:\n",
        "   - The model achieved an **R-squared of 1.000**, indicating that it explains nearly all the variability in the dependent variable \\( Y \\).\n",
        "   - The **Adjusted R-squared** is also 1.000, confirming minimal overfitting in this specific model.\n",
        "   - The low **AIC (271.8)** and **BIC (282.2)** suggest a good model fit relative to the number of predictors used.\n",
        "\n",
        "2. **Significant Predictors**:\n",
        "   - The model retained three predictors: \\( X7 \\), \\( X2 \\), and \\( X9 \\).\n",
        "   - **\\( X7 \\)**:\n",
        "     - Coefficient = **1.9859**, with a very small standard error and a t-value of 214.68, suggesting it is the most influential and statistically significant predictor (\\( p < 0.001 \\)).\n",
        "   - **\\( X2 \\)**:\n",
        "     - Coefficient = **0.2084**, significant at the \\( 5\\% \\) level (\\( p = 0.047 \\)).\n",
        "   - **\\( X9 \\)**:\n",
        "     - Coefficient = **0.0022**, not statistically significant (\\( p = 0.103 \\)). Despite being included by the stepwise procedure, its contribution to the model appears marginal.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - \\( X7 \\) is the dominant driver of \\( Y \\), while \\( X2 \\) provides moderate additional explanatory power.\n",
        "   - The inclusion of \\( X9 \\) may reflect overfitting to the specific sample data, given its lack of statistical significance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Lasso Regression Results**\n",
        "1. **Overview**:\n",
        "   - Lasso regression selected \\( X9 \\) and \\( X10 \\) as the only predictors with nonzero coefficients, emphasizing its feature selection capability.\n",
        "   - \\( X9 \\): Coefficient = **0.2612**, indicating a moderately strong and positive relationship with \\( Y \\).\n",
        "   - \\( X10 \\): Coefficient = **-0.0116**, indicating a weak negative relationship with \\( Y \\).\n",
        "\n",
        "2. **Differences from OLS**:\n",
        "   - Lasso penalizes less influential predictors more aggressively, leading to the exclusion of \\( X7 \\), which was the strongest predictor in the OLS model.\n",
        "   - This discrepancy highlights that Lasso prioritizes simplicity by reducing the model complexity at the cost of ignoring some impactful predictors.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - Lasso's emphasis on \\( X9 \\) (which was not significant in OLS) suggests that it might capture unique variance or interactions not reflected in the OLS selection.\n",
        "   - However, the exclusion of \\( X7 \\), a key predictor in OLS, raises concerns about underfitting in the Lasso model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Comparison and Insights**:\n",
        "1. **Predictor Selection**:\n",
        "   - The forward stepwise method in OLS selected \\( X7 \\), \\( X2 \\), and \\( X9 \\), while Lasso included only \\( X9 \\) and \\( X10 \\).\n",
        "   - The contrast demonstrates the trade-offs between the two methods:\n",
        "     - OLS focuses on maximizing explanatory power.\n",
        "     - Lasso emphasizes parsimony by eliminating less important predictors.\n",
        "\n",
        "2. **Model Fit**:\n",
        "   - The OLS model shows superior fit (high \\( R^2 \\) and statistical significance for predictors).\n",
        "   - Lasso sacrifices fit for simplicity, which might be beneficial if overfitting is a concern in larger datasets or for generalizability.\n",
        "\n",
        "3. **Conclusion**:\n",
        "   - The OLS model is more interpretable and effective for this dataset, given its high \\( R^2 \\) and inclusion of statistically significant predictors.\n",
        "   - Lasso's results might be more suitable for high-dimensional data or scenarios where multicollinearity is an issue."
      ],
      "metadata": {
        "id": "hOZRjQEJlDcx"
      }
    }
  ]
}