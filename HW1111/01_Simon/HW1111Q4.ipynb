{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "(9.) In this exercise, we will predict the number of applications received\n",
        "using the other variables in the College data set."
      ],
      "metadata": {
        "id": "ectfiVHNlgg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Split the data set into a training set and a test set."
      ],
      "metadata": {
        "id": "mhlBentBll_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuxdDJOlfQx",
        "outputId": "54de465f-b762-4017-b379-513df08bae37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training set size: 621\n",
            "Testing set size: 156\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'path/to/your/college.csv' with the actual path\n",
        "college_df = pd.read_csv('/content/drive/My Drive/College.csv')\n",
        "\n",
        "# Drop the collge names\n",
        "college_df = college_df.drop(college_df.columns[0], axis=1)\n",
        "\n",
        "# Transform 'Private' column using Label Encoding\n",
        "encoder = LabelEncoder()\n",
        "college_df['Private'] = encoder.fit_transform(college_df['Private'])\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = college_df.drop('Apps', axis=1) # Assuming 'Apps' is the target variable\n",
        "y = college_df['Apps']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # You can adjust test_size and random_state\n",
        "\n",
        "# Now you have:\n",
        "# X_train, y_train: Training data (features and target)\n",
        "# X_test, y_test: Testing data (features and target)\n",
        "\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Testing set size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Fit a linear model using least squares on the training set, and\n",
        "report the test error obtained."
      ],
      "metadata": {
        "id": "8okB-NmjmS0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the test error (mean squared error)\n",
        "test_error = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Test Error (Mean Squared Error):\", test_error)\n",
        "\n",
        "# R-squared\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LofCQ1aQmTYH",
        "outputId": "b585ac35-e11b-4e74-bf0b-dabf0623d231"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error (Mean Squared Error): 1492443.3790390454\n",
            "R-squared: 0.8877583168400976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Fit a ridge regression model on the training set, with λ chosen\n",
        "by cross-validation. Report the test error obtained."
      ],
      "metadata": {
        "id": "UMIzTqRbtoYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# Create a Ridge regression model with cross-validation to choose lambda (alpha)\n",
        "ridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)  # You can adjust the alphas and cv\n",
        "\n",
        "# Fit the model on the training data\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "# Calculate the test error (mean squared error) for ridge regression\n",
        "test_error_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"Test Error (Ridge Regression, Mean Squared Error):\", test_error_ridge)\n",
        "\n",
        "# R-squared for Ridge Regression\n",
        "r_squared_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(\"R-squared (Ridge Regression):\", r_squared_ridge)\n",
        "\n",
        "# You can also print the chosen alpha (lambda) value:\n",
        "print(\"Chosen alpha (lambda) for Ridge Regression:\", ridge_model.alpha_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ATOZNitrzX",
        "outputId": "d1bea41e-e438-40d0-fa9f-3fad9e472765"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error (Ridge Regression, Mean Squared Error): 1478572.8112797\n",
            "R-squared (Ridge Regression): 0.8888014759264375\n",
            "Chosen alpha (lambda) for Ridge Regression: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Fit a lasso model on the training set, with λ chosen by crossvalidation.\n",
        "Report the test error obtained, along with the number\n",
        "of non-zero coefficient estimates."
      ],
      "metadata": {
        "id": "Gwo7w1K8t4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# Create a Lasso regression model with cross-validation to choose lambda (alpha)\n",
        "lasso_model = LassoCV(cv=5)  # You can adjust cv\n",
        "\n",
        "# Fit the model on the training data\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "\n",
        "# Calculate the test error (mean squared error) for lasso regression\n",
        "test_error_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Test Error (Lasso Regression, Mean Squared Error):\", test_error_lasso)\n",
        "\n",
        "# R-squared for Lasso Regression\n",
        "r_squared_lasso = r2_score(y_test, y_pred_lasso)\n",
        "print(\"R-squared (Lasso Regression):\", r_squared_lasso)\n",
        "\n",
        "# Count the number of non-zero coefficients\n",
        "non_zero_coefficients = sum(lasso_model.coef_ != 0)\n",
        "print(\"Number of non-zero coefficients:\", non_zero_coefficients)\n",
        "\n",
        "# You can also print the chosen alpha (lambda) value:\n",
        "print(\"Chosen alpha (lambda) for Lasso Regression:\", lasso_model.alpha_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7SOHT9Wt6Sd",
        "outputId": "d5c4f363-fdea-4d5a-8591-d25bbf07b9c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error (Lasso Regression, Mean Squared Error): 1587020.0176529174\n",
            "R-squared (Lasso Regression): 0.8806455236482635\n",
            "Number of non-zero coefficients: 7\n",
            "Chosen alpha (lambda) for Lasso Regression: 14444.597843675856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Fit a PCR model on the training set, with M chosen by crossvalidation.\n",
        "Report the test error obtained, along with the value\n",
        "of M selected by cross-validation."
      ],
      "metadata": {
        "id": "KthrEKd4uFkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a pipeline with PCA and linear regression\n",
        "pipeline = Pipeline([\n",
        "    ('pca', PCA()),\n",
        "    ('linear', LinearRegression())\n",
        "])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "param_grid = {\n",
        "    'pca__n_components': list(range(1, X_train.shape[1] + 1))  # Try different numbers of principal components\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object to find the best M (number of principal components)\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model using cross-validation\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best M and the corresponding test error\n",
        "best_m = grid_search.best_params_['pca__n_components']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred_pcr = best_model.predict(X_test_scaled)\n",
        "test_error_pcr = mean_squared_error(y_test, y_pred_pcr)\n",
        "\n",
        "print(\"Best M (number of principal components):\", best_m)\n",
        "print(\"Test Error (PCR, Mean Squared Error):\", test_error_pcr)\n",
        "\n",
        "# R-squared for PCR\n",
        "r_squared_pcr = r2_score(y_test, y_pred_pcr)\n",
        "print(\"R-squared (PCR):\", r_squared_pcr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4B32hQouGLf",
        "outputId": "05d4acac-2040-494d-91d5-fa763ad6fbea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best M (number of principal components): 17\n",
            "Test Error (PCR, Mean Squared Error): 1492443.379039024\n",
            "R-squared (PCR): 0.8877583168400992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Fit a PLS model on the training set, with M chosen by crossvalidation.\n",
        "Report the test error obtained, along with the value\n",
        "of M selected by cross-validation."
      ],
      "metadata": {
        "id": "P_cuYj0_uN8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a pipeline with PLS and linear regression\n",
        "pipeline_pls = Pipeline([\n",
        "    ('pls', PLSRegression()),\n",
        "    #('linear', LinearRegression()) # Not strictly needed as PLSRegression already does regression\n",
        "])\n",
        "\n",
        "# Define the parameter grid for cross-validation\n",
        "param_grid_pls = {\n",
        "    'pls__n_components': list(range(1, min(X_train.shape[0], X_train.shape[1]) + 1))  # Try different numbers of components\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object to find the best M (number of components)\n",
        "grid_search_pls = GridSearchCV(pipeline_pls, param_grid_pls, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model using cross-validation\n",
        "grid_search_pls.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best M and the corresponding test error\n",
        "best_m_pls = grid_search_pls.best_params_['pls__n_components']\n",
        "best_model_pls = grid_search_pls.best_estimator_\n",
        "\n",
        "y_pred_pls = best_model_pls.predict(X_test_scaled)\n",
        "test_error_pls = mean_squared_error(y_test, y_pred_pls)\n",
        "\n",
        "print(\"Best M (number of components) for PLS:\", best_m_pls)\n",
        "print(\"Test Error (PLS, Mean Squared Error):\", test_error_pls)\n",
        "\n",
        "# R-squared for PLS\n",
        "r_squared_pls = r2_score(y_test, y_pred_pls)\n",
        "print(\"R-squared (PLS):\", r_squared_pls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAUaDyhSuOa3",
        "outputId": "98112310-39c0-47ec-c1b3-d6afea4cfe48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best M (number of components) for PLS: 17\n",
            "Test Error (PLS, Mean Squared Error): 1492443.379039025\n",
            "R-squared (PLS): 0.8877583168400992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) Comment on the results obtained. How accurately can we predict\n",
        "the number of college applications received? Is there much\n",
        "difference among the test errors resulting from these five approaches?"
      ],
      "metadata": {
        "id": "EqCLJ7NJzbMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Linear Regression\n",
        "- Test Error (MSE): 1,492,443\n",
        "- R-squared: 0.888\n",
        "- Linear regression serves as a baseline model. It explains approximately 88.8% of the variability in the number of applications. The relatively high test error suggests there is still room for improvement.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Ridge Regression\n",
        "- Test Error (MSE): 1,478,572\n",
        "- R-squared: 0.889\n",
        "- Ridge regression improves slightly on the test error compared to linear regression, reducing it by a small margin(~14,000).\n",
        "- The chosen α (regularization parameter) of 10.0 indicates moderate penalization of large coefficients, which likely helps in reducing overfitting while maintaining predictive accuracy.\n",
        "- Ridge regression performs marginally better than linear regression, but the improvement is not substantial.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Lasso Regression\n",
        "- Test Error (MSE): 1,587,020\n",
        "- R-squared: 0.881\n",
        "- Lasso regression performs slightly worse than both linear and ridge regression, with a higher test error and a lower $R^2$.\n",
        "- It retains 7 non-zero coefficients, indicating significant feature reduction, but this simplification comes at the cost of accuracy.\n",
        "- The chosen α is quite large (14,444.6), leading to aggressive regularization and potentially omitting valuable predictors.\n",
        "- While Lasso is effective for feature selection, it sacrifices some predictive accuracy compared to ridge and linear regression.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Principal Component Regression (PCR)\n",
        "- Test Error (MSE): 1,492,443\n",
        "- R-squared: 0.888\n",
        "- With the optimal number of components (M = 17), PCR achieves the same test error and $R^2$ as linear regression. This suggests that the principal components capture similar predictive information as the original variables.\n",
        "- PCR does not provide additional predictive accuracy but may be useful in high-dimensional settings to handle multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Partial Least Squares (PLS)\n",
        "- Test Error (MSE): 1,492,443\n",
        "- R-squared: 0.888\n",
        "- Similar to PCR, PLS achieves identical performance metrics to linear regression, with (M = 17) components. This indicates no added benefit in terms of predictive accuracy for this dataset.\n",
        "- Like PCR, PLS provides no significant advantage but could be beneficial in datasets with correlated predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparative Analysis\n",
        "1. Predictive Accuracy:\n",
        "   - All five methods yield similar test errors and $R^2$ values, with differences in MSE of less than \\( 110,000 \\). This suggests the models predict the number of college applications received with reasonable accuracy, explaining about 88% of the variance.\n",
        "   - Ridge regression performs marginally better, indicating that slight regularization improves predictive performance.\n",
        "   - Lasso performs slightly worse due to aggressive penalization, likely discarding important features.\n",
        "\n",
        "2. Differences in Test Errors:\n",
        "   - The differences among test errors are not substantial, with the smallest (Ridge: 1,478,572) and largest (Lasso: 1,587,020) differing by about \\( 7.3\\% \\).\n",
        "   - This suggests the dataset does not strongly benefit from regularization or dimensionality reduction.\n",
        "\n",
        "3. Feature Selection and Dimensionality Reduction:\n",
        "   - Ridge retains all predictors, while Lasso reduces the number to 7, offering a simpler model at the cost of accuracy.\n",
        "   - PCR and PLS reduce the dimensionality to 17 components, matching the performance of linear regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "- Accuracy: The models provide reasonably accurate predictions, explaining about 88% of the variability in college applications.\n",
        "- Differences Among Methods: The test errors across methods are quite close, with ridge regression slightly outperforming the others.\n"
      ],
      "metadata": {
        "id": "Nm9yejYm1WB4"
      }
    }
  ]
}